{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d9326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a639dd1",
   "metadata": {},
   "source": [
    "## SGD weight optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336709a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class model_together(nn.Module):\n",
    "    def __init__(self, num_dtypes):\n",
    "        super(model_together, self).__init__()\n",
    "        self.weights = nn.Linear(num_dtypes, 3)\n",
    "        #self.fc = nn.Linear(3, 1)\n",
    "    \n",
    "    def forward(self,xb):\n",
    "        out = self.weights(xb)\n",
    "        #out = self.fc(F.relu(out))\n",
    "        out = torch.diagonal(out, 0, dim1=-2, dim2=-1)\n",
    "        #out = F.softmax(out, dim=1)\n",
    "        \n",
    "        return out\n",
    "def init_params(size, std=1.0): return (torch.rand(size)*std).requires_grad_()   \n",
    "def init_model(model, num_dtypes):\n",
    "    with torch.no_grad():\n",
    "        params = init_params((3,num_dtypes))\n",
    "        params = params / params.sum(1, keepdim=True)                     \n",
    "        model.weights.weight.copy_(params).float()\n",
    "        model.weights.bias.fill_(0)\n",
    "\n",
    "def norm_params(model):\n",
    "    with torch.no_grad():\n",
    "        # Push positive before scaling:\n",
    "        #model.model.weight[model.model.weight < 0] = 0.\n",
    "        #weights_norm = model.model.weight / model.model.weight.sum(1, keepdim=True)\n",
    "        weights_norm = F.softmax(model.weights.weight, dim=1)\n",
    "        model.weights.weight.copy_(weights_norm).float()\n",
    "        model.weights.bias.fill_(0)\n",
    "        \n",
    "# this is ok to be with numpy but then need to be converted to tensors\n",
    "def get_val_set(x, y, classes, percentage = 0.1):\n",
    "    np.random.seed(42)  \n",
    "    x_train = np.array([]).reshape(0,x.shape[1])\n",
    "    y_train = np.array([]).reshape(0,y.shape[1])\n",
    "    x_val = np.array([]).reshape(0,x.shape[1])\n",
    "    y_val = np.array([]).reshape(0,y.shape[1])\n",
    "    for c in classes:\n",
    "        indexes = np.where(y.argmax(axis=1) == c)[0]\n",
    "        np.random.shuffle(indexes)\n",
    "        len_val = int(percentage * len(indexes))\n",
    "        len_train = len(indexes) - len_val\n",
    "        index_train = indexes[0:len_train]\n",
    "        index_val = indexes[len_train:]\n",
    "        x_train = np.concatenate([x_train, x[index_train,...]], axis=0)\n",
    "        y_train = np.concatenate([y_train, y[index_train]], axis=0)\n",
    "        x_val = np.concatenate([x_val, x[index_val,...]], axis=0)\n",
    "        y_val = np.concatenate([y_val, y[index_val]], axis=0)\n",
    "    \n",
    "    index_train = list(range(x_train.shape[0]))\n",
    "    index_val = list(range(x_val.shape[0]))\n",
    "    np.random.shuffle(index_train)\n",
    "    np.random.shuffle(index_val)\n",
    "    \n",
    "    return x_train[index_train,...],y_train[index_train], x_val[index_val,...], y_val[index_val]\n",
    "\n",
    "def train_model(model, dls, optimizer, criterion, num_epochs):\n",
    "# copy code from the other notebook\n",
    "    # Start training\n",
    "    best_epoch = 0\n",
    "    best_acc = 100\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_labels_auc = []\n",
    "    val_preds_auc = []\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    train_preds_auc = []\n",
    "    train_labels_auc = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    losses = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "    sizes = {'train': 0, 'val': 0}\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        batch_loss = 0\n",
    "        sizes = {}\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            sizes[phase] = 0\n",
    "            running_loss = 0\n",
    "            running_corrects = 0\n",
    "            for inputs, labels in dls[phase]:\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.type(torch.LongTensor)\n",
    "                #labels = F.one_hot(labels, num_classes=3)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    #import pdb; pdb.set_trace()\n",
    "                    loss = criterion(outputs, labels)\n",
    "                if phase == 'val':\n",
    "                    val_preds += list(preds.numpy())\n",
    "                    val_labels += list(labels.numpy())\n",
    "                    val_labels_auc += [labels.numpy()]\n",
    "                    val_preds_auc += [preds.numpy()]\n",
    "\n",
    "                \n",
    "                # Accumulating the loss over time\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "                sizes[phase] += inputs.size(0)\n",
    "                \n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    train_preds += list(preds.numpy())\n",
    "                    train_labels += list(labels.numpy())\n",
    "                    train_labels_auc += [labels.numpy()]\n",
    "                    train_preds_auc += [preds.numpy()]\n",
    "                    # Getting gradients w.r.t. parameters\n",
    "                    loss.backward()\n",
    "                    # Updating parameters\n",
    "                    optimizer.step()\n",
    "                    #norm_params(model)\n",
    "                \n",
    "            epoch_loss = running_loss / sizes[phase]\n",
    "            epoch_acc = running_corrects.item() / sizes[phase]\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    phase, epoch_loss, epoch_acc))\n",
    "            losses[phase].append(epoch_loss)\n",
    "            if phase == 'val' and epoch_loss < best_acc:\n",
    "                best_epoch = epoch\n",
    "                best_acc = epoch_loss\n",
    "                #best_auc = epoch_auc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        # Lr scheduler\n",
    "        #lr_scheduler.step()\n",
    "                    \n",
    "    print('Best epoch {}'.format(best_epoch))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    '''\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(num_epochs)),losses['train'], label='train', color='blue')\n",
    "    plt.plot(list(range(num_epochs)),losses['val'], label='val', color='orange')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    '''\n",
    "    norm_params(model)\n",
    "    return model\n",
    "\n",
    "def get_alphas_SGD(df, classes, data_types):\n",
    "    x_train = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        x = [[],[],[]]\n",
    "        for d_type in data_types:\n",
    "            if row['Has ' + d_type] != -1:\n",
    "                luad_new = row[d_type+ ' Prob LUAD']\n",
    "                hlt_new = row[d_type+ ' Prob HLT']\n",
    "                lusc_new = row[d_type+ ' Prob LUSC']\n",
    "                #luad_new = luad / (luad + hlt +lusc)\n",
    "                #hlt_new = hlt / (luad + hlt +lusc)\n",
    "                #lusc_new = lusc / (luad + hlt +lusc)\n",
    "                x[0].append(luad_new)\n",
    "                x[1].append(hlt_new)\n",
    "                x[2].append(lusc_new)\n",
    "            else:\n",
    "                x[0].append(0)\n",
    "                x[1].append(0)\n",
    "                x[2].append(0)\n",
    "        x_train.append(x)\n",
    "    x_train = np.asarray(x_train)\n",
    "    real = df['Real'].values\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=0)\n",
    "    \n",
    "    for train_index, test_index  in sss.split(x_train, real):\n",
    "        train_x, train_y = x_train[train_index,:,:], real[train_index]\n",
    "        val_x, val_y = x_train[test_index,:,:], real[test_index]\n",
    "    \n",
    "    dl_train = DataLoader(list(zip(train_x,train_y)), batch_size=32, shuffle=True)\n",
    "    dl_val = DataLoader(list(zip(val_x,val_y)), batch_size=32, shuffle=False)\n",
    "    dls = {'train': dl_train, 'val': dl_val}\n",
    "    \n",
    "    model = model_together(len(data_types))\n",
    "    init_model(model, len(data_types))\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.01)\n",
    "    loss = nn.CrossEntropyLoss() \n",
    "    \n",
    "    model = train_model(model, dls, optimizer, loss, num_epochs=5)\n",
    "\n",
    "    # a\n",
    "    # once the optimization has been carried out\n",
    "    weights = model.weights.weight.detach().numpy()\n",
    "    \n",
    "    alphas = {}\n",
    "    index_dtype = 0\n",
    "    for d_type in data_types:\n",
    "        alphas[d_type] = []\n",
    "        for i in range(len(classes)):\n",
    "            alphas[d_type].append(weights[i,index_dtype])\n",
    "        index_dtype += 1\n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d15a31e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def integration_model(data_types: List[str], datasets: List[str], name: str, path: str, \n",
    "                      fusion_type='probs', use_alphas=False, m_alphas=None) -> None:\n",
    "    for d in datasets:\n",
    "        writer = pd.ExcelWriter(path+'data_integration_model_'+d+'_'+fusion_type+'_'+name+'.xlsx', engine='openpyxl')\n",
    "        data = pd.read_excel('data_integration_noBDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021a7152",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def integration_model(data_types: List[str], datasets: List[str], name: str, path: str, \n",
    "                      fusion_type='probs', use_alphas=False, m_alphas=None) -> None:\n",
    "    for d in datasets:\n",
    "        writer = pd.ExcelWriter(path+'data_integration_model_'+d+'_'+fusion_type+'_'+name+'.xlsx', engine='openpyxl')\n",
    "        data = pd.read_excel('data_integration_'+d+'.xlsx',\n",
    "              sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "        \n",
    "        if use_alphas:\n",
    "            print('Getting alphas from training set...')\n",
    "            data_train = pd.read_excel('data_integration_train.xlsx',\n",
    "              sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "            splits_alphas = {}\n",
    "            for df_name, df in data_train.items():\n",
    "                #print(df_name)\n",
    "                alphas = get_alphas_SGD(df, [0,1,2], data_types)\n",
    "                splits_alphas[df_name] = alphas\n",
    "                print(alphas)\n",
    "            print('Saving alphas...')\n",
    "            f = open(path+'data_integration_model_'+d+'_'+fusion_type+'_'+name+'_alphas.pkl',\"wb\")\n",
    "            pickle.dump(splits_alphas,f)\n",
    "            f.close()\n",
    "\n",
    "        for df_name, df in data.items():\n",
    "            integration_probs = {\n",
    "            'LUAD': [],\n",
    "            'HLT': [],\n",
    "            'LUSC': []\n",
    "            }\n",
    "            integration_preds = []\n",
    "            for _, row in tqdm(df.iterrows()):\n",
    "                local_probs = {\n",
    "                    'LUAD': [],\n",
    "                    'HLT': [],\n",
    "                    'LUSC': []\n",
    "                }\n",
    "                local_preds = []\n",
    "                for d_type in data_types:\n",
    "                    if row['Has ' + d_type] != -1:\n",
    "                        luad = row[d_type+ ' Prob LUAD']\n",
    "                        hlt = row[d_type+ ' Prob HLT']\n",
    "                        lusc = row[d_type+ ' Prob LUSC']\n",
    "                        luad_new = luad / (luad + hlt +lusc)\n",
    "                        hlt_new = hlt / (luad + hlt +lusc)\n",
    "                        lusc_new = lusc / (luad + hlt +lusc)\n",
    "                        local_probs['LUAD'].append(luad_new)\n",
    "                        local_probs['HLT'].append(hlt_new)\n",
    "                        local_probs['LUSC'].append(lusc_new)\n",
    "                        local_preds.append(row[d_type + ' Pred'])\n",
    "                    elif use_alphas:\n",
    "                        local_probs['LUAD'].append(0)\n",
    "                        local_probs['HLT'].append(0)\n",
    "                        local_probs['LUSC'].append(0)\n",
    "                        \n",
    "                if fusion_type == 'probs':\n",
    "                    if use_alphas:\n",
    "                        if m_alphas:\n",
    "                        #alphas_manual = [0.65, 0.35]\n",
    "                            alphas_manual = m_alphas\n",
    "                            luad_prob = integrate_probs(local_probs['LUAD'], 0, alphas_manual)\n",
    "                            hlt_prob = integrate_probs(local_probs['HLT'], 1, alphas_manual)\n",
    "                            lusc_prob = integrate_probs(local_probs['LUSC'], 2, alphas_manual)\n",
    "                        else:\n",
    "                            luad_prob_new = integrate_probs(local_probs['LUAD'], 0, splits_alphas[df_name])\n",
    "                            hlt_prob_new = integrate_probs(local_probs['HLT'], 1, splits_alphas[df_name])\n",
    "                            lusc_prob_new = integrate_probs(local_probs['LUSC'], 2, splits_alphas[df_name])\n",
    "                            luad_prob = luad_prob_new/ (luad_prob_new + hlt_prob_new + lusc_prob_new)\n",
    "                            lusc_prob = lusc_prob_new/ (luad_prob_new + hlt_prob_new + lusc_prob_new)\n",
    "                            hlt_prob = hlt_prob_new/ (luad_prob_new + hlt_prob_new + lusc_prob_new)\n",
    "                    else:\n",
    "                        luad_prob = integrate_probs(local_probs['LUAD'])\n",
    "                        hlt_prob = integrate_probs(local_probs['HLT'])\n",
    "                        lusc_prob = integrate_probs(local_probs['LUSC'])\n",
    "                    integration_probs['LUAD'].append(luad_prob)\n",
    "                    integration_probs['HLT'].append(hlt_prob)\n",
    "                    integration_probs['LUSC'].append(lusc_prob)\n",
    "\n",
    "                    pred = np.argmax([luad_prob,hlt_prob,lusc_prob], axis=0)\n",
    "                    integration_preds.append(pred)\n",
    "\n",
    "                elif fusion_type == 'preds':\n",
    "                    if len(local_preds) == 2:\n",
    "                        # if there are only two predictions, we need to fuse the probabilities\n",
    "                        luad_prob = integrate_probs(local_probs['LUAD'])\n",
    "                        hlt_prob = integrate_probs(local_probs['HLT'])\n",
    "                        lusc_prob = integrate_probs(local_probs['LUSC'])\n",
    "                        \n",
    "                        integration_probs['LUAD'].append(luad_prob)\n",
    "                        integration_probs['HLT'].append(hlt_prob)\n",
    "                        integration_probs['LUSC'].append(lusc_prob)\n",
    "                        \n",
    "                        pred = np.argmax([luad_prob,hlt_prob,lusc_prob], axis=0)\n",
    "                        \n",
    "                    else:\n",
    "                        pred = integrate_preds(local_preds)\n",
    "                        if pred == 0:\n",
    "                            luad_prob = 1\n",
    "                            hlt_prob = 0\n",
    "                            lusc_prob = 0\n",
    "                        elif pred == 1:\n",
    "                            luad_prob = 0\n",
    "                            hlt_prob = 1\n",
    "                            lusc_prob = 0\n",
    "                        else:\n",
    "                            luad_prob = 0\n",
    "                            hlt_prob = 0\n",
    "                            lusc_prob = 1\n",
    "\n",
    "                        integration_probs['LUAD'].append(luad_prob)\n",
    "                        integration_probs['HLT'].append(hlt_prob)\n",
    "                        integration_probs['LUSC'].append(lusc_prob)\n",
    "                    \n",
    "                \n",
    "                    integration_preds.append(pred)\n",
    "\n",
    "            \n",
    "            for cls in integration_probs.keys():\n",
    "                df['Integration Prob '+ cls] = integration_probs[cls]\n",
    "\n",
    "            df['Integration Pred'] = integration_preds\n",
    "\n",
    "            # save to sheet\n",
    "            df.to_excel(writer, sheet_name='split_'+str(df_name), index=False)\n",
    "\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f454288",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "integration_model(data_types=[\"WSI\", \"RNA\", \"miRNA\", \"CNV\", 'DNA'],\n",
    "                  datasets=['test', 'train'], name=\"SGD-all_sources\",\n",
    "                  path='results_SGD/', fusion_type='probs', use_alphas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da653670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "data_model = pd.read_excel('../result_files/data_integration_model_test_probs_SGD-all_sources.xlsx',\n",
    "              sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "\n",
    "accs = {\n",
    "    'WSI': [],\n",
    "    'RNA': [],\n",
    "    'miRNA': [],\n",
    "    'CNV': [],\n",
    "    'DNA': [],\n",
    "    'Integration': {'WSI': [], 'RNA': [], 'miRNA': [], 'CNV': [], 'DNA': [], 'Integration': []}\n",
    "}\n",
    "\n",
    "f1_scores = {\n",
    "    'WSI': [],\n",
    "    'RNA': [],\n",
    "    'miRNA': [],\n",
    "    'CNV': [],\n",
    "    'DNA': [],\n",
    "    'Integration': {'WSI': [], 'RNA': [], 'miRNA': [], 'CNV': [], 'DNA': [],'Integration': []}\n",
    "}\n",
    "\n",
    "aucs = {\n",
    "    'WSI': [],\n",
    "    'RNA': [],\n",
    "    'miRNA': [],\n",
    "    'CNV': [],\n",
    "    'DNA': [],\n",
    "    'Integration': {'WSI': [], 'RNA': [], 'miRNA': [], 'CNV': [], 'DNA': [], 'Integration': []}\n",
    "}\n",
    "\n",
    "auprcs = {\n",
    "    'WSI': [],\n",
    "    'RNA': [],\n",
    "    'miRNA': [],\n",
    "    'CNV': [],\n",
    "    'DNA': [],\n",
    "    'Integration': {'WSI': [], 'RNA': [], 'miRNA': [], 'CNV': [], 'DNA': [], 'Integration': []}\n",
    "}\n",
    "\n",
    "for d_type in ['WSI', 'RNA', 'miRNA', 'CNV', 'DNA']:\n",
    "        for df_name, df in data_model.items():\n",
    "            if d_type != 'Integration':\n",
    "                df_only = df.loc[df['Has '+ d_type] != -1]\n",
    "            else:\n",
    "                df_only = df\n",
    "            probs = [[x,y,z] for x,y,z in zip(df_only[d_type + ' Prob LUAD'], df_only[d_type + ' Prob HLT'], df_only[d_type + ' Prob LUSC'])]\n",
    "            preds = df_only[d_type + ' Pred'].values\n",
    "            probs = np.asarray(probs)\n",
    "            real = df_only['Real'].values\n",
    "            real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "            acc = accuracy_score(real, preds)*100\n",
    "            f1 = f1_score(real, preds, average='weighted')*100\n",
    "            auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "            accs[d_type].append(acc)\n",
    "            f1_scores[d_type].append(f1)\n",
    "            aucs[d_type].append(auc)\n",
    "            aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "            aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "            aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "            auprcs[d_type].append(np.mean([aucpr1, aucpr2, aucpr3]))\n",
    "            \n",
    "            # integration\n",
    "            probs_int = [[x,y,z] for x,y,z in zip(df_only['Integration Prob LUAD'], df_only['Integration Prob HLT'], df_only['Integration Prob LUSC'])]\n",
    "            preds_int = df_only['Integration Pred'].values\n",
    "            probs_int = np.asarray(probs_int)\n",
    "            real = df_only['Real'].values\n",
    "            real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "            acc_int = accuracy_score(real, preds_int)*100\n",
    "            f1_int = f1_score(real, preds_int, average='weighted')*100\n",
    "            auc_int = roc_auc_score(real, probs_int, multi_class='ovr')\n",
    "            accs['Integration'][d_type].append(acc_int)\n",
    "            f1_scores['Integration'][d_type].append(f1_int)\n",
    "            aucs['Integration'][d_type].append(auc_int)\n",
    "            aucpr1 = average_precision_score(real_binarized[:, 0], probs_int[:, 0])\n",
    "            aucpr2 = average_precision_score(real_binarized[:, 1], probs_int[:, 1])\n",
    "            aucpr3 = average_precision_score(real_binarized[:, 2], probs_int[:, 2])\n",
    "            auprcs['Integration'][d_type].append(np.mean([aucpr1, aucpr2, aucpr3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a2ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d_type in ['WSI', 'RNA', 'miRNA', 'CNV', 'DNA']:\n",
    "    print(d_type + ' ACC: {}+-{}'.format(np.mean(accs[d_type]),np.std(accs[d_type])))\n",
    "    print(d_type + ' ACC: {}+-{}'.format(np.mean(accs['Integration'][d_type]),np.std(accs['Integration'][d_type])))\n",
    "    print(5*'-')\n",
    "print(10*'-')\n",
    "\n",
    "for d_type in ['WSI', 'RNA', 'miRNA', 'CNV', 'DNA']:  \n",
    "    print(d_type + ' F1: {}+-{}'.format(np.mean(f1_scores[d_type]),np.std(f1_scores[d_type])))\n",
    "    print(d_type + ' F1: {}+-{}'.format(np.mean(f1_scores['Integration'][d_type]),np.std(f1_scores['Integration'][d_type])))\n",
    "    print(5*'-')\n",
    "print(10*'-')\n",
    "\n",
    "for d_type in ['WSI', 'RNA', 'miRNA', 'CNV', 'DNA']:\n",
    "    print(d_type + ' AUC: {}+-{}'.format(np.mean(aucs[d_type]),np.std(aucs[d_type])))\n",
    "    print(d_type + ' AUC: {}+-{}'.format(np.mean(aucs['Integration'][d_type]),np.std(aucs['Integration'][d_type])))\n",
    "    print(5*'-')\n",
    "print(10*'-')\n",
    "\n",
    "for d_type in ['WSI', 'RNA', 'miRNA', 'CNV', 'DNA']:\n",
    "    print(d_type + ' AUPRC: {}+-{}'.format(np.mean(auprcs[d_type]),np.std(auprcs[d_type])))\n",
    "    print(d_type + ' AUPRC: {}+-{}'.format(np.mean(auprcs['Integration'][d_type]),np.std(auprcs['Integration'][d_type])))\n",
    "    print(5*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ad3e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "accs = []\n",
    "f1_scores = []\n",
    "aucs = []\n",
    "auprcs = []\n",
    "data_model = pd.read_excel('../result_files/data_integration_model_test_probs_SGD-all_sources.xlsx',\n",
    "              sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "\n",
    "d_type = 'Integration'\n",
    "for df_name, df in data_model.items():\n",
    "\n",
    "    df_only = df\n",
    "    probs = [[x,y,z] for x,y,z in zip(df_only[d_type + ' Prob LUAD'], df_only[d_type + ' Prob HLT'], df_only[d_type + ' Prob LUSC'])]\n",
    "    probs = np.asarray(probs)\n",
    "    preds = df_only[d_type + ' Pred'].values\n",
    "\n",
    "    real = df_only['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "\n",
    "    aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "    aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "    aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "    \n",
    "    acc = accuracy_score(real, preds)*100\n",
    "    f1 = f1_score(real, preds, average='weighted')*100\n",
    "    auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "    accs.append(acc)\n",
    "    f1_scores.append(f1)\n",
    "    aucs.append(auc)\n",
    "    auprcs.append(np.mean([aucpr1, aucpr2, aucpr3]))\n",
    "\n",
    "print(d_type + ' ACC: {}+-{}'.format(np.mean(accs),np.std(accs)))\n",
    "print(d_type + ' F1: {}+-{}'.format(np.mean(f1_scores),np.std(f1_scores)))\n",
    "print(d_type + ' AUC: {}+-{}'.format(np.mean(aucs),np.std(aucs)))\n",
    "print(d_type + ' AUPRC: {}+-{}'.format(np.mean(auprcs),np.std(auprcs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0995f57b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for d_type in ['WSI', 'RNA', 'miRNA', 'CNV', 'DNA']:\n",
    "    print(d_type + ' ACC: {}+-{}'.format(np.mean(accs[d_type]),np.std(accs[d_type])))\n",
    "    print(d_type + ' ACC: {}+-{}'.format(np.mean(accs['Integration'][d_type]),np.std(accs['Integration'][d_type])))\n",
    "    print(5*'-')\n",
    "print(10*'-')\n",
    "\n",
    "for d_type in ['WSI', 'RNA', 'miRNA', 'CNV', 'DNA']:  \n",
    "    print(d_type + ' F1: {}+-{}'.format(np.mean(f1_scores[d_type]),np.std(f1_scores[d_type])))\n",
    "    print(d_type + ' F1: {}+-{}'.format(np.mean(f1_scores['Integration'][d_type]),np.std(f1_scores['Integration'][d_type])))\n",
    "    print(5*'-')\n",
    "print(10*'-')\n",
    "\n",
    "for d_type in ['WSI', 'RNA', 'miRNA', 'CNV', 'DNA']:\n",
    "    print(d_type + ' AUC: {}+-{}'.format(np.mean(aucs[d_type]),np.std(aucs[d_type])))\n",
    "    print(d_type + ' AUC: {}+-{}'.format(np.mean(aucs['Integration'][d_type]),np.std(aucs['Integration'][d_type])))\n",
    "    print(5*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d036d539",
   "metadata": {},
   "source": [
    "## check best integration two sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('results_SGD/two-sources-integration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8a2e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = ['WSI', 'RNA', 'miRNA', 'CNV', 'DNA']\n",
    "\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "for d_type1 in data_types[i:]:\n",
    "    for d_type2 in data_types[j:]:\n",
    "        name = d_type1 + '-' + d_type2\n",
    "        integration_model(data_types=[d_type1, d_type2],\n",
    "                          datasets=['test'], name=name,\n",
    "                          path='results_SGD/two-sources-integration/',\n",
    "                          fusion_type='probs', use_alphas=True)\n",
    "    i += 1\n",
    "    j = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431889bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "data_types = ['WSI', 'RNA', 'miRNA', 'CNV', 'DNA']\n",
    "\n",
    "accs = {}\n",
    "f1_scores = {}\n",
    "aucs = {}\n",
    "aucprcs = {}\n",
    "\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "for d_type1 in data_types[i:]:\n",
    "    for d_type2 in data_types[j:]:\n",
    "        name = d_type1 + '-' + d_type2\n",
    "        data_model = pd.read_excel('../result_files/two-sources-integration/data_integration_model_test_probs_'+name+'.xlsx',\n",
    "              sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "        \n",
    "        accs[name] = {}\n",
    "        f1_scores[name] = {}\n",
    "        aucs[name] = {}\n",
    "        aucprcs[name] = {}\n",
    "        \n",
    "        accs[name]['Integration'] = []\n",
    "        accs[name][d_type1+'Int'] = []\n",
    "        accs[name][d_type2+'Int'] = []\n",
    "        \n",
    "        f1_scores[name]['Integration'] = []\n",
    "        f1_scores[name][d_type1+'Int'] = []\n",
    "        f1_scores[name][d_type2+'Int'] = []\n",
    "        \n",
    "        aucs[name]['Integration'] = []\n",
    "        aucs[name][d_type1+'Int'] = []\n",
    "        aucs[name][d_type2+'Int'] = []\n",
    "        \n",
    "        aucprcs[name]['Integration'] = []\n",
    "        aucprcs[name][d_type1+'Int'] = []\n",
    "        aucprcs[name][d_type2+'Int'] = []\n",
    "        \n",
    "        for df_name, df in data_model.items():\n",
    "            # take those where the two sources has data\n",
    "            df_only = df.loc[(df['Has '+ d_type1] != -1) | (df['Has ' + d_type2] != -1)]\n",
    "            df_dt1 = df.loc[df['Has '+ d_type1] != -1]\n",
    "            df_dt2 = df.loc[df['Has '+ d_type2] != -1]\n",
    "            probs = [[x,y,z] for x,y,z in zip(df_only['Integration Prob LUAD'], df_only['Integration Prob HLT'], df_only['Integration Prob LUSC'])]\n",
    "            probs = np.asarray(probs)\n",
    "            preds = df_only['Integration Pred'].values\n",
    "            real = df_only['Real'].values\n",
    "            acc = accuracy_score(real, preds)*100\n",
    "            f1 = f1_score(real, preds, average='weighted')*100\n",
    "            auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "            real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "\n",
    "            aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "            aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "            aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "            \n",
    "            accs[name]['Integration'].append(acc)\n",
    "            f1_scores[name]['Integration'].append(f1)\n",
    "            aucs[name]['Integration'].append(auc)\n",
    "            aucprcs[name]['Integration'].append([aucpr1,aucpr2,aucpr3])\n",
    "            \n",
    "            # dtype1\n",
    "            probs = [[x,y,z] for x,y,z in zip(df_dt1['Integration Prob LUAD'], df_dt1['Integration Prob HLT'], df_dt1['Integration Prob LUSC'])]\n",
    "            probs = np.asarray(probs)\n",
    "            preds = df_dt1['Integration Pred'].values\n",
    "            real = df_dt1['Real'].values\n",
    "            real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "            acc = accuracy_score(real, preds)*100\n",
    "            f1 = f1_score(real, preds, average='weighted')*100\n",
    "            auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "            \n",
    "            aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "            aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "            aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "            \n",
    "            accs[name][d_type1+'Int'].append(acc)\n",
    "            f1_scores[name][d_type1+'Int'].append(f1)\n",
    "            aucs[name][d_type1+'Int'].append(auc)\n",
    "            aucprcs[name][d_type1+'Int'].append(np.mean([aucpr1,aucpr2,aucpr3]))\n",
    "            \n",
    "            # dtype2\n",
    "            probs = [[x,y,z] for x,y,z in zip(df_dt2['Integration Prob LUAD'], df_dt2['Integration Prob HLT'], df_dt2['Integration Prob LUSC'])]\n",
    "            preds = df_dt2['Integration Pred'].values\n",
    "            probs = np.asarray(probs)\n",
    "            real = df_dt2['Real'].values\n",
    "            real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "            acc = accuracy_score(real, preds)*100\n",
    "            f1 = f1_score(real, preds, average='weighted')*100\n",
    "            auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "            aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "            aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "            aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "            \n",
    "            accs[name][d_type2+'Int'].append(acc)\n",
    "            f1_scores[name][d_type2+'Int'].append(f1)\n",
    "            aucs[name][d_type2+'Int'].append(auc)\n",
    "            aucprcs[name][d_type2+'Int'].append(np.mean([aucpr1,aucpr2,aucpr3]))\n",
    "            \n",
    "    i += 1\n",
    "    j = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bfe092",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 1\n",
    "\n",
    "for d_type1 in data_types[i:]:\n",
    "    for d_type2 in data_types[j:]:\n",
    "        print(5*'-')\n",
    "        name = d_type1 + '-' + d_type2\n",
    "        print(name + ' ACC: {}+-{}'.format(np.mean(accs[name]['Integration']),np.std(accs[name]['Integration'])))\n",
    "        print(d_type1+'Int' + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type1+'Int']),np.std(accs[name][d_type1+'Int'])))\n",
    "        print(d_type2+'Int' + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type2+'Int']),np.std(accs[name][d_type2+'Int'])))\n",
    "        \n",
    "    i += 1\n",
    "    j = i + 1\n",
    "print(10*'-')\n",
    "\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "for d_type1 in data_types[i:]:\n",
    "    for d_type2 in data_types[j:]:\n",
    "        name = d_type1 + '-' + d_type2\n",
    "        print(5*'-')\n",
    "        print(name + ' F1: {}+-{}'.format(np.mean(f1_scores[name]['Integration']),np.std(f1_scores[name]['Integration'])))\n",
    "        print(d_type1+'Int' + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type1+'Int']),np.std(f1_scores[name][d_type1+'Int'])))\n",
    "        print(d_type2+'Int' + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type2+'Int']),np.std(f1_scores[name][d_type2+'Int'])))\n",
    "    i += 1\n",
    "    j = i + 1\n",
    "print(10*'-')\n",
    "\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "for d_type1 in data_types[i:]:\n",
    "    for d_type2 in data_types[j:]:\n",
    "        print(5*'-')\n",
    "        name = d_type1 + '-' + d_type2\n",
    "        print(name + ' AUC: {}+-{}'.format(np.mean(aucs[name]['Integration']),np.std(aucs[name]['Integration'])))\n",
    "        print(d_type1+'Int' + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type1+'Int']),np.std(aucs[name][d_type1+'Int'])))\n",
    "        print(d_type2+'Int' + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type2+'Int']),np.std(aucs[name][d_type2+'Int'])))\n",
    "    i += 1\n",
    "    j = i + 1\n",
    "\n",
    "i = 0\n",
    "j = 1\n",
    "for d_type1 in data_types[i:]:\n",
    "    for d_type2 in data_types[j:]:\n",
    "        print(5*'-')\n",
    "        name = d_type1 + '-' + d_type2\n",
    "        print(name + ' AUPRC: {}+-{}'.format(np.mean(aucprcs[name]['Integration']),np.std(aucprcs[name]['Integration'])))\n",
    "        print(d_type1+'Int' + ' AUPRC: {}+-{}'.format(np.mean(aucprcs[name][d_type1+'Int']),np.std(aucprcs[name][d_type1+'Int'])))\n",
    "        print(d_type2+'Int' + ' AUPRC: {}+-{}'.format(np.mean(aucprcs[name][d_type2+'Int']),np.std(aucprcs[name][d_type2+'Int'])))\n",
    "    i += 1\n",
    "    j = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc563e9",
   "metadata": {},
   "source": [
    "## check best integration three sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('results_SGD/three-sources-integration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = ['WSI', 'RNA', 'miRNA', 'CNV', 'DNA']\n",
    "\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "\n",
    "accs = {}\n",
    "f1_scores = {}\n",
    "aucs = {}\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            name = d_type1 + '-' + d_type2 + '-' + d_type3\n",
    "            integration_model(data_types=[d_type1, d_type2, d_type3],\n",
    "                          datasets=['test'], name=name,\n",
    "                          path='results_SGD/three-sources-integration/',\n",
    "                          fusion_type='probs', use_alphas=True)\n",
    "        \n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e43b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_types = ['WSI', 'RNA', 'miRNA', 'CNV', 'DNA']\n",
    "data_types2 = copy.copy(data_types)\n",
    "data_types3 = copy.copy(data_types)\n",
    "\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "\n",
    "accs = {}\n",
    "f1_scores = {}\n",
    "aucs = {}\n",
    "aucprcs = {}\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            name = d_type1 + '-' + d_type2 + '-' + d_type3\n",
    "            data_model = pd.read_excel('../result_files/three-sources-integration/data_integration_model_test_probs_'+name+'.xlsx',\n",
    "              sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "        \n",
    "            accs[name] = {}\n",
    "            f1_scores[name] = {}\n",
    "            aucs[name] = {}\n",
    "            aucprcs[name] = {}\n",
    "\n",
    "            accs[name]['Integration'] = []\n",
    "            accs[name][d_type1+'Int'] = []\n",
    "            accs[name][d_type2+'Int'] = []\n",
    "            accs[name][d_type3+'Int'] = []\n",
    "\n",
    "            f1_scores[name]['Integration'] = []\n",
    "            f1_scores[name][d_type1+'Int'] = []\n",
    "            f1_scores[name][d_type2+'Int'] = []\n",
    "            f1_scores[name][d_type3+'Int'] = []\n",
    "\n",
    "            aucs[name]['Integration'] = []\n",
    "            aucs[name][d_type1+'Int'] = []\n",
    "            aucs[name][d_type2+'Int'] = []\n",
    "            aucs[name][d_type3+'Int'] = []\n",
    "            \n",
    "            aucprcs[name]['Integration'] = []\n",
    "            aucprcs[name][d_type1+'Int'] = []\n",
    "            aucprcs[name][d_type2+'Int'] = []\n",
    "            aucprcs[name][d_type3+'Int'] = []\n",
    "            \n",
    "            for df_name, df in data_model.items():\n",
    "                # take those where the two sources has data\n",
    "                df_only = df.loc[(df['Has '+ d_type1] != -1) | (df['Has ' + d_type2] != -1) | (df['Has ' + d_type3] != -1)]\n",
    "                df_dt1 = df.loc[df['Has '+ d_type1] != -1]\n",
    "                df_dt2 = df.loc[df['Has '+ d_type2] != -1]\n",
    "                df_dt3 = df.loc[df['Has '+ d_type3] != -1]\n",
    "                \n",
    "                probs = [[x,y,z] for x,y,z in zip(df_only['Integration Prob LUAD'], df_only['Integration Prob HLT'], df_only['Integration Prob LUSC'])]\n",
    "                preds = df_only['Integration Pred'].values\n",
    "                probs = np.asarray(probs)\n",
    "                real = df_only['Real'].values\n",
    "                real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "                acc = accuracy_score(real, preds)*100\n",
    "                f1 = f1_score(real, preds, average='weighted')*100\n",
    "                auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "                accs[name]['Integration'].append(acc)\n",
    "                f1_scores[name]['Integration'].append(f1)\n",
    "                aucs[name]['Integration'].append(auc)\n",
    "                \n",
    "                aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                aucprcs[name]['Integration'].append(np.mean([aucpr1,aucpr2,aucpr3]))\n",
    "                \n",
    "                # dtype1\n",
    "                probs = [[x,y,z] for x,y,z in zip(df_dt1['Integration Prob LUAD'], df_dt1['Integration Prob HLT'], df_dt1['Integration Prob LUSC'])]\n",
    "                preds = df_dt1['Integration Pred'].values\n",
    "                probs = np.asarray(probs)\n",
    "                real = df_dt1['Real'].values\n",
    "                real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "                acc = accuracy_score(real, preds)*100\n",
    "                f1 = f1_score(real, preds, average='weighted')*100\n",
    "                auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "\n",
    "                accs[name][d_type1+'Int'].append(acc)\n",
    "                f1_scores[name][d_type1+'Int'].append(f1)\n",
    "                aucs[name][d_type1+'Int'].append(auc)\n",
    "                \n",
    "                aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                aucprcs[name][d_type1+'Int'].append(np.mean([aucpr1,aucpr2,aucpr3]))\n",
    "                \n",
    "                # dtype2\n",
    "                probs = [[x,y,z] for x,y,z in zip(df_dt2['Integration Prob LUAD'], df_dt2['Integration Prob HLT'], df_dt2['Integration Prob LUSC'])]\n",
    "                preds = df_dt2['Integration Pred'].values\n",
    "                probs = np.asarray(probs)\n",
    "                real = df_dt2['Real'].values\n",
    "                real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "                acc = accuracy_score(real, preds)*100\n",
    "                f1 = f1_score(real, preds, average='weighted')*100\n",
    "                auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "\n",
    "                accs[name][d_type2+'Int'].append(acc)\n",
    "                f1_scores[name][d_type2+'Int'].append(f1)\n",
    "                aucs[name][d_type2+'Int'].append(auc)\n",
    "                \n",
    "                aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                aucprcs[name][d_type2+'Int'].append(np.mean([aucpr1,aucpr2,aucpr3]))\n",
    "                \n",
    "                \n",
    "                # dtype3\n",
    "                probs = [[x,y,z] for x,y,z in zip(df_dt3['Integration Prob LUAD'], df_dt3['Integration Prob HLT'], df_dt3['Integration Prob LUSC'])]\n",
    "                preds = df_dt3['Integration Pred'].values\n",
    "                probs = np.asarray(probs)\n",
    "                real = df_dt3['Real'].values\n",
    "                real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "                acc = accuracy_score(real, preds)*100\n",
    "                f1 = f1_score(real, preds, average='weighted')*100\n",
    "                auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "\n",
    "                accs[name][d_type3+'Int'].append(acc)\n",
    "                f1_scores[name][d_type3+'Int'].append(f1)\n",
    "                aucs[name][d_type3+'Int'].append(auc)\n",
    "                \n",
    "                aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                aucprcs[name][d_type3+'Int'].append(np.mean([aucpr1,aucpr2,aucpr3]))\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7967cf9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            print(5*'-')\n",
    "            name = d_type1 + '-' + d_type2 + '-' + d_type3\n",
    "            print(name + ' ACC: {}+-{}'.format(np.mean(accs[name]['Integration']),np.std(accs[name]['Integration'])))\n",
    "            print(d_type1+'Int' + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type1+'Int']),np.std(accs[name][d_type1+'Int'])))\n",
    "            print(d_type2+'Int' + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type2+'Int']),np.std(accs[name][d_type2+'Int'])))\n",
    "            print(d_type3+'Int' + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type3+'Int']),np.std(accs[name][d_type3+'Int'])))\n",
    "        \n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "\n",
    "print(10*'-')\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            print(5*'-')\n",
    "            name = d_type1 + '-' + d_type2 + '-' + d_type3\n",
    "            print(name + ' F1: {}+-{}'.format(np.mean(f1_scores[name]['Integration']),np.std(f1_scores[name]['Integration'])))\n",
    "            print(d_type1+'Int' + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type1+'Int']),np.std(f1_scores[name][d_type1+'Int'])))\n",
    "            print(d_type2+'Int' + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type2+'Int']),np.std(f1_scores[name][d_type2+'Int'])))\n",
    "            print(d_type3+'Int' + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type3+'Int']),np.std(f1_scores[name][d_type3+'Int'])))\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "\n",
    "print(10*'-')\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            print(5*'-')\n",
    "            name = d_type1 + '-' + d_type2 + '-' + d_type3\n",
    "            print(name + ' AUC: {}+-{}'.format(np.mean(aucs[name]['Integration']),np.std(aucs[name]['Integration'])))\n",
    "            print(d_type1+'Int' + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type1+'Int']),np.std(aucs[name][d_type1+'Int'])))\n",
    "            print(d_type2+'Int' + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type2+'Int']),np.std(aucs[name][d_type2+'Int'])))\n",
    "            print(d_type3+'Int' + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type3+'Int']),np.std(aucs[name][d_type3+'Int'])))\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "    \n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            print(5*'-')\n",
    "            name = d_type1 + '-' + d_type2 + '-' + d_type3\n",
    "            print(name + ' AUPRC: {}+-{}'.format(np.mean(aucprcs[name]['Integration']),np.std(aucprcs[name]['Integration'])))\n",
    "            print(d_type1+'Int' + ' AUPRC: {}+-{}'.format(np.mean(aucprcs[name][d_type1+'Int']),np.std(aucprcs[name][d_type1+'Int'])))\n",
    "            print(d_type2+'Int' + ' AUPRC: {}+-{}'.format(np.mean(aucprcs[name][d_type2+'Int']),np.std(aucprcs[name][d_type2+'Int'])))\n",
    "            print(d_type3+'Int' + ' AUPRC: {}+-{}'.format(np.mean(aucprcs[name][d_type3+'Int']),np.std(aucprcs[name][d_type3+'Int'])))\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb951ad8",
   "metadata": {},
   "source": [
    "## check best integration four sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab32d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('../result_files/four-sources-integration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac8a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = ['WSI', 'RNA', 'miRNA', 'CNV', 'DNA']\n",
    "\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "z = 3\n",
    "\n",
    "accs = {}\n",
    "f1_scores = {}\n",
    "aucs = {}\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            for d_type4 in data_types[z:]:\n",
    "                name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "                integration_model(data_types=[d_type1, d_type2, d_type3, d_type4],\n",
    "                          datasets=['test'], name=name,\n",
    "                          path='results_SGD/four-sources-integration/',\n",
    "                          fusion_type='probs', use_alphas=True)\n",
    "            z += 1\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "    z = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d6e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = ['WSI', 'RNA', 'miRNA', 'CNV', 'DNA']\n",
    "\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "z = 3\n",
    "\n",
    "accs = {}\n",
    "f1_scores = {}\n",
    "aucs = {}\n",
    "aucprcs = {}\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            for d_type4 in data_types[z:]:\n",
    "                name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "                data_model = pd.read_excel('../result_files/four-sources-integration/data_integration_model_test_probs_'+name+'.xlsx',\n",
    "                  sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "        \n",
    "                accs[name] = {}\n",
    "                f1_scores[name] = {}\n",
    "                aucs[name] = {}\n",
    "                aucprcs[name] = {}\n",
    "                \n",
    "                accs[name]['Integration'] = []\n",
    "                accs[name][d_type1+'Int'] = []\n",
    "                accs[name][d_type2+'Int'] = []\n",
    "                accs[name][d_type3+'Int'] = []\n",
    "                accs[name][d_type4+'Int'] = []\n",
    "                \n",
    "                f1_scores[name]['Integration'] = []\n",
    "                f1_scores[name][d_type1+'Int'] = []\n",
    "                f1_scores[name][d_type2+'Int'] = []\n",
    "                f1_scores[name][d_type3+'Int'] = []\n",
    "                f1_scores[name][d_type4+'Int'] = []\n",
    "                \n",
    "                aucs[name]['Integration'] = []\n",
    "                aucs[name][d_type1+'Int'] = []\n",
    "                aucs[name][d_type2+'Int'] = []\n",
    "                aucs[name][d_type3+'Int'] = []\n",
    "                aucs[name][d_type4+'Int'] = []\n",
    "                \n",
    "                aucprcs[name]['Integration'] = []\n",
    "                aucprcs[name][d_type1+'Int'] = []\n",
    "                aucprcs[name][d_type2+'Int'] = []\n",
    "                aucprcs[name][d_type3+'Int'] = []\n",
    "                aucprcs[name][d_type4+'Int'] = []\n",
    "                \n",
    "                for df_name, df in data_model.items():\n",
    "                    # take those where the four sources has data\n",
    "                    df_only = df.loc[(df['Has '+ d_type1] != -1) | (df['Has ' + d_type2] != -1) | (df['Has ' + d_type3] != -1) | (df['Has ' + d_type4] != -1)]\n",
    "                    df_dt1 = df.loc[df['Has '+ d_type1] != -1]\n",
    "                    df_dt2 = df.loc[df['Has '+ d_type2] != -1]\n",
    "                    df_dt3 = df.loc[df['Has '+ d_type3] != -1]\n",
    "                    df_dt4 = df.loc[df['Has '+ d_type4] != -1]\n",
    "                    \n",
    "                    probs = [[x,y,z] for x,y,z in zip(df_only['Integration Prob LUAD'], df_only['Integration Prob HLT'], df_only['Integration Prob LUSC'])]\n",
    "                    probs = np.asarray(probs)\n",
    "                    preds = df_only['Integration Pred'].values\n",
    "                    real = df_only['Real'].values\n",
    "                    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "                    acc = accuracy_score(real, preds)*100\n",
    "                    f1 = f1_score(real, preds, average='weighted')*100\n",
    "                    auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "                    accs[name]['Integration'].append(acc)\n",
    "                    f1_scores[name]['Integration'].append(f1)\n",
    "                    aucs[name]['Integration'].append(auc)\n",
    "                    \n",
    "                    aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                    aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                    aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                    aucprcs[name]['Integration'].append(np.mean([aucpr1,aucpr2,aucpr3]))\n",
    "                \n",
    "                    # dtype1\n",
    "                    probs = [[x,y,z] for x,y,z in zip(df_dt1['Integration Prob LUAD'], df_dt1['Integration Prob HLT'], df_dt1['Integration Prob LUSC'])]\n",
    "                    probs = np.asarray(probs)\n",
    "                    preds = df_dt1['Integration Pred'].values\n",
    "                    real = df_dt1['Real'].values\n",
    "                    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "                    acc = accuracy_score(real, preds)*100\n",
    "                    f1 = f1_score(real, preds, average='weighted')*100\n",
    "                    auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "\n",
    "                    accs[name][d_type1+'Int'].append(acc)\n",
    "                    f1_scores[name][d_type1+'Int'].append(f1)\n",
    "                    aucs[name][d_type1+'Int'].append(auc)\n",
    "                    \n",
    "                    aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                    aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                    aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                    aucprcs[name][d_type1+'Int'].append(np.mean([aucpr1,aucpr2,aucpr3]))\n",
    "                    # dtype2\n",
    "                    probs = [[x,y,z] for x,y,z in zip(df_dt2['Integration Prob LUAD'], df_dt2['Integration Prob HLT'], df_dt2['Integration Prob LUSC'])]\n",
    "                    preds = df_dt2['Integration Pred'].values\n",
    "                    probs = np.asarray(probs)\n",
    "                    real = df_dt2['Real'].values\n",
    "                    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "                    acc = accuracy_score(real, preds)*100\n",
    "                    f1 = f1_score(real, preds, average='weighted')*100\n",
    "                    auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "\n",
    "                    accs[name][d_type2+'Int'].append(acc)\n",
    "                    f1_scores[name][d_type2+'Int'].append(f1)\n",
    "                    aucs[name][d_type2+'Int'].append(auc)\n",
    "                    \n",
    "                    aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                    aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                    aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                    aucprcs[name][d_type2+'Int'].append(np.mean([aucpr1,aucpr2,aucpr3]))\n",
    "                    \n",
    "                    # dtype3\n",
    "                    probs = [[x,y,z] for x,y,z in zip(df_dt3['Integration Prob LUAD'], df_dt3['Integration Prob HLT'], df_dt3['Integration Prob LUSC'])]\n",
    "                    preds = df_dt3['Integration Pred'].values\n",
    "                    probs = np.asarray(probs)\n",
    "                    real = df_dt3['Real'].values\n",
    "                    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "                    acc = accuracy_score(real, preds)*100\n",
    "                    f1 = f1_score(real, preds, average='weighted')*100\n",
    "                    auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "\n",
    "                    accs[name][d_type3+'Int'].append(acc)\n",
    "                    f1_scores[name][d_type3+'Int'].append(f1)\n",
    "                    aucs[name][d_type3+'Int'].append(auc)\n",
    "                    \n",
    "                    aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                    aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                    aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                    aucprcs[name][d_type3+'Int'].append(np.mean([aucpr1,aucpr2,aucpr3]))\n",
    "                    \n",
    "                    # dtype4\n",
    "                    probs = [[x,y,z] for x,y,z in zip(df_dt4['Integration Prob LUAD'], df_dt4['Integration Prob HLT'], df_dt4['Integration Prob LUSC'])]\n",
    "                    probs = np.asarray(probs)\n",
    "                    preds = df_dt4['Integration Pred'].values\n",
    "                    real = df_dt4['Real'].values\n",
    "                    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "                    acc = accuracy_score(real, preds)*100\n",
    "                    f1 = f1_score(real, preds, average='weighted')*100\n",
    "                    auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "\n",
    "                    accs[name][d_type4+'Int'].append(acc)\n",
    "                    f1_scores[name][d_type4+'Int'].append(f1)\n",
    "                    aucs[name][d_type4+'Int'].append(auc)\n",
    "                    \n",
    "                    aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                    aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                    aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                    aucprcs[name][d_type4+'Int'].append(np.mean([aucpr1,aucpr2,aucpr3]))\n",
    "            z += 1\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "    z = j + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b793458e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "z = 3\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            for d_type4 in data_types[z:]:\n",
    "                print(5*'-')\n",
    "                name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "                print(name + ' ACC: {}+-{}'.format(np.mean(accs[name]['Integration']),np.std(accs[name]['Integration'])))\n",
    "                print(d_type1+'Int' + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type1+'Int']),np.std(accs[name][d_type1+'Int'])))\n",
    "                print(d_type2+'Int' + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type2+'Int']),np.std(accs[name][d_type2+'Int'])))\n",
    "                print(d_type3+'Int' + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type3+'Int']),np.std(accs[name][d_type3+'Int'])))\n",
    "                print(d_type4+'Int' + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type4+'Int']),np.std(accs[name][d_type4+'Int'])))\n",
    "            z += 1\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "    z = j + 1\n",
    "\n",
    "print(10*'-')\n",
    "\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "z = 3\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            for d_type4 in data_types[z:]:\n",
    "                print(5*'-')\n",
    "                name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "                print(name + ' F1: {}+-{}'.format(np.mean(f1_scores[name]['Integration']),np.std(f1_scores[name]['Integration'])))\n",
    "                print(d_type1+'Int' + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type1+'Int']),np.std(f1_scores[name][d_type1+'Int'])))\n",
    "                print(d_type2+'Int' + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type2+'Int']),np.std(f1_scores[name][d_type2+'Int'])))\n",
    "                print(d_type3+'Int' + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type3+'Int']),np.std(f1_scores[name][d_type3+'Int'])))\n",
    "                print(d_type4+'Int' + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type4+'Int']),np.std(f1_scores[name][d_type4+'Int'])))\n",
    "            z += 1\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "    z = j + 1\n",
    "\n",
    "print(10*'-')\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "z = 3\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            for d_type4 in data_types[z:]:\n",
    "                print(5*'-')\n",
    "                name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "                print(name + ' AUC: {}+-{}'.format(np.mean(aucs[name]['Integration']),np.std(aucs[name]['Integration'])))\n",
    "                print(d_type1+'Int' + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type1+'Int']),np.std(aucs[name][d_type1+'Int'])))\n",
    "                print(d_type2+'Int' + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type2+'Int']),np.std(aucs[name][d_type2+'Int'])))\n",
    "                print(d_type3+'Int' + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type3+'Int']),np.std(aucs[name][d_type3+'Int'])))\n",
    "                print(d_type4+'Int' + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type4+'Int']),np.std(aucs[name][d_type4+'Int'])))\n",
    "            z += 1\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "    z = j + 1\n",
    "\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "z = 3\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            for d_type4 in data_types[z:]:\n",
    "                print(5*'-')\n",
    "                name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "                print(name + ' AUPRC: {}+-{}'.format(np.mean(aucprcs[name]['Integration']),np.std(aucprcs[name]['Integration'])))\n",
    "                print(d_type1+'Int' + ' AUPRC: {}+-{}'.format(np.mean(aucprcs[name][d_type1+'Int']),np.std(aucprcs[name][d_type1+'Int'])))\n",
    "                print(d_type2+'Int' + ' AUPRC: {}+-{}'.format(np.mean(aucprcs[name][d_type2+'Int']),np.std(aucprcs[name][d_type2+'Int'])))\n",
    "                print(d_type3+'Int' + ' AUPRC: {}+-{}'.format(np.mean(aucprcs[name][d_type3+'Int']),np.std(aucprcs[name][d_type3+'Int'])))\n",
    "                print(d_type4+'Int' + ' AUPRC: {}+-{}'.format(np.mean(aucprcs[name][d_type4+'Int']),np.std(aucprcs[name][d_type4+'Int'])))\n",
    "            z += 1\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "    z = j + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0441230f",
   "metadata": {},
   "source": [
    "## check best integration five sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90918939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "data_model = pd.read_excel('../result_files/data_integration_model_test_probs_SGD-all_sources.xlsx',\n",
    "              sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "\n",
    "accs = {\n",
    "    'WSI': [],\n",
    "    'RNA': [],\n",
    "    'miRNA': [],\n",
    "    'CNV': [],\n",
    "    'DNA': [],\n",
    "    'Integration': []\n",
    "}\n",
    "\n",
    "f1_scores = {\n",
    "    'WSI': [],\n",
    "    'RNA': [],\n",
    "    'miRNA': [],\n",
    "    'CNV': [],\n",
    "    'DNA': [],\n",
    "    'Integration': []\n",
    "}\n",
    "\n",
    "aucs = {\n",
    "    'WSI': [],\n",
    "    'RNA': [],\n",
    "    'miRNA': [],\n",
    "    'CNV': [],\n",
    "    'DNA': [],\n",
    "    'Integration': []\n",
    "}\n",
    "\n",
    "auprcs = {\n",
    "    'WSI': [],\n",
    "    'RNA': [],\n",
    "    'miRNA': [],\n",
    "    'CNV': [],\n",
    "    'DNA': [],\n",
    "    'Integration': []\n",
    "}\n",
    "\n",
    "sizes = 0\n",
    "lengths['all'] = {'luad': 0, 'hlt': 0, 'lusc': 0}\n",
    "for df_name, df in data_model.items():\n",
    "    \n",
    "    df_only = df.loc[(df['Has WSI'] != -1) | (df['Has RNA'] != -1) | (df['Has miRNA'] != -1) | (df['Has CNV'] != -1) | (df['Has DNA'] != -1)]\n",
    "    df_dt1 = df.loc[df['Has RNA'] != -1]\n",
    "    df_dt2 = df.loc[df['Has WSI'] != -1]\n",
    "    df_dt3 = df.loc[df['Has miRNA'] != -1]\n",
    "    df_dt4 = df.loc[df['Has CNV'] != -1]\n",
    "    df_dt5 = df.loc[df['Has DNA'] != -1]\n",
    "    print(df_only.shape[0])\n",
    "    sizes += df_only.shape[0]\n",
    "    \n",
    "    luad_s = len(np.where(real == 0)[0])\n",
    "    hlt_s = len(np.where(real == 1)[0])\n",
    "    lusc_s = len(np.where(real == 2)[0])\n",
    "\n",
    "    lengths['all']['luad'] += luad_s\n",
    "    lengths['all']['hlt'] += hlt_s\n",
    "    lengths['all']['lusc'] += lusc_s\n",
    "    # RNA\n",
    "    probs = [[x,y,z] for x,y,z in zip(df_dt1['RNA Prob LUAD'], df_dt1['RNA Prob HLT'], df_dt1['RNA Prob LUSC'])]\n",
    "    probs = np.asarray(probs)\n",
    "    preds = df_dt1['RNA Pred'].values\n",
    "    real = df_dt1['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "    \n",
    "    acc = accuracy_score(real, preds)*100\n",
    "    f1 = f1_score(real, preds, average='weighted')*100\n",
    "    try:\n",
    "        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "        aucs['RNA'].append(auc)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "        auprcs['RNA'].append(np.mean(array))\n",
    "    except:\n",
    "        pass\n",
    "                \n",
    "    accs['RNA'].append(acc)\n",
    "    f1_scores['RNA'].append(f1)\n",
    "    \n",
    "    # WSI\n",
    "    probs = [[x,y,z] for x,y,z in zip(df_dt2['WSI Prob LUAD'], df_dt2['WSI Prob HLT'], df_dt2['WSI Prob LUSC'])]\n",
    "    probs = np.asarray(probs)\n",
    "    preds = df_dt2['WSI Pred'].values\n",
    "    real = df_dt2['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "    acc = accuracy_score(real, preds)*100\n",
    "    f1 = f1_score(real, preds, average='weighted')*100\n",
    "    try:\n",
    "        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "        aucs['WSI'].append(auc)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "        auprcs['WSI'].append(np.mean(array))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    accs['WSI'].append(acc)\n",
    "    f1_scores['WSI'].append(f1)\n",
    "    \n",
    "    # miRNA\n",
    "    probs = [[x,y,z] for x,y,z in zip(df_dt3['miRNA Prob LUAD'], df_dt3['miRNA Prob HLT'], df_dt3['miRNA Prob LUSC'])]\n",
    "    probs = np.asarray(probs)\n",
    "    preds = df_dt3['miRNA Pred'].values\n",
    "    real = df_dt3['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "    acc = accuracy_score(real, preds)*100\n",
    "    f1 = f1_score(real, preds, average='weighted')*100\n",
    "    try:\n",
    "        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "        aucs['miRNA'].append(auc)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "        auprcs['miRNA'].append(np.mean(array))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    accs['miRNA'].append(acc)\n",
    "    f1_scores['miRNA'].append(f1)\n",
    "    \n",
    "    # CNV\n",
    "    probs = [[x,y,z] for x,y,z in zip(df_dt4['CNV Prob LUAD'], df_dt4['CNV Prob HLT'], df_dt4['CNV Prob LUSC'])]\n",
    "    probs = np.asarray(probs)\n",
    "    preds = df_dt4['CNV Pred'].values\n",
    "    real = df_dt4['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "    acc = accuracy_score(real, preds)*100\n",
    "    f1 = f1_score(real, preds, average='weighted')*100\n",
    "    try:\n",
    "        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "        aucs['CNV'].append(auc)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "        auprcs['CNV'].append(np.mean(array))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    accs['CNV'].append(acc)\n",
    "    f1_scores['CNV'].append(f1)\n",
    "    \n",
    "    # DNA\n",
    "    probs = [[x,y,z] for x,y,z in zip(df_dt5['DNA Prob LUAD'], df_dt5['DNA Prob HLT'], df_dt5['DNA Prob LUSC'])]\n",
    "    probs = np.asarray(probs)\n",
    "    preds = df_dt5['DNA Pred'].values\n",
    "    real = df_dt5['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "    acc = accuracy_score(real, preds)*100\n",
    "    f1 = f1_score(real, preds, average='weighted')*100\n",
    "    try:\n",
    "        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "        aucs['DNA'].append(auc)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "        auprcs['DNA'].append(np.mean(array))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    accs['DNA'].append(acc)\n",
    "    f1_scores['DNA'].append(f1)\n",
    "    \n",
    "    # integration\n",
    "    probs_int = [[x,y,z] for x,y,z in zip(df_only['Integration Prob LUAD'], df_only['Integration Prob HLT'], df_only['Integration Prob LUSC'])]\n",
    "    preds_int = df_only['Integration Pred'].values\n",
    "    probs_int = np.asarray(probs_int)\n",
    "    real = df_only['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "    acc_int = accuracy_score(real, preds_int)*100\n",
    "    f1_int = f1_score(real, preds_int, average='weighted')*100\n",
    "    try:\n",
    "        auc_int = roc_auc_score(real, probs_int, multi_class='ovr')\n",
    "        aucs['Integration'].append(auc_int)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        aucpr1 = average_precision_score(real_binarized[:, 0], probs_int[:, 0])\n",
    "        aucpr2 = average_precision_score(real_binarized[:, 1], probs_int[:, 1])\n",
    "        aucpr3 = average_precision_score(real_binarized[:, 2], probs_int[:, 2])\n",
    "        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "        auprcs['Integration'].append(np.mean(array))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    accs['Integration'].append(acc_int)\n",
    "    f1_scores['Integration'].append(f1_int)\n",
    "\n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20525abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d_type in ['Integration','WSI', 'RNA', 'miRNA', 'CNV', 'DNA']:\n",
    "    print(d_type + ' ACC: {}+-{}'.format(np.mean(accs[d_type]),np.std(accs[d_type])))\n",
    "    print(5*'-')\n",
    "print(10*'-')\n",
    "\n",
    "for d_type in ['Integration','WSI', 'RNA', 'miRNA', 'CNV', 'DNA']:  \n",
    "    print(d_type + ' F1: {}+-{}'.format(np.mean(f1_scores[d_type]),np.std(f1_scores[d_type])))\n",
    "    print(5*'-')\n",
    "print(10*'-')\n",
    "\n",
    "for d_type in ['Integration','WSI', 'RNA', 'miRNA', 'CNV', 'DNA']:\n",
    "    print(d_type + ' AUC: {}+-{}'.format(np.mean(aucs[d_type]),np.std(aucs[d_type])))\n",
    "    print(5*'-')\n",
    "    \n",
    "print(10*'-')\n",
    "\n",
    "for d_type in ['Integration','WSI', 'RNA', 'miRNA', 'CNV', 'DNA']:\n",
    "    print(d_type + ' AUPRC: {}+-{}'.format(np.mean(auprcs[d_type]),np.std(auprcs[d_type])))\n",
    "    print(5*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf705a",
   "metadata": {},
   "source": [
    "## check best integration two sources without NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d4693",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "accs = {}\n",
    "f1_scores = {}\n",
    "aucs = {}\n",
    "auprcs = {}\n",
    "lengths = {}\n",
    "\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "\n",
    "\n",
    "for d_type1 in data_types[i:]:\n",
    "    for d_type2 in data_types[j:]:\n",
    "        name = d_type1 + '-' + d_type2\n",
    "        data_model = pd.read_excel('../result_files/two-sources-integration/data_integration_model_test_probs_'+name+'.xlsx',\n",
    "              sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "        \n",
    "        accs[name] = {}\n",
    "        f1_scores[name] = {}\n",
    "        aucs[name] = {}\n",
    "        auprcs[name] = {}\n",
    "        \n",
    "        accs[name]['Integration'] = []\n",
    "        accs[name][d_type1+'Int'] = []\n",
    "        accs[name][d_type2+'Int'] = []\n",
    "        \n",
    "        f1_scores[name]['Integration'] = []\n",
    "        f1_scores[name][d_type1+'Int'] = []\n",
    "        f1_scores[name][d_type2+'Int'] = []\n",
    "        \n",
    "        aucs[name]['Integration'] = []\n",
    "        aucs[name][d_type1+'Int'] = []\n",
    "        aucs[name][d_type2+'Int'] = []\n",
    "        \n",
    "        auprcs[name]['Integration'] = []\n",
    "        auprcs[name][d_type1+'Int'] = []\n",
    "        auprcs[name][d_type2+'Int'] = []\n",
    "        \n",
    "        lengths[name] = {'luad': 0, 'hlt': 0, 'lusc': 0}\n",
    "        \n",
    "        for df_name, df in data_model.items():\n",
    "            # take those where the two sources has data\n",
    "            df_only = df.loc[(df['Has '+ d_type1] != -1) & (df['Has ' + d_type2] != -1)]\n",
    "            df_dt1 = df_only.loc[df_only['Has '+ d_type1] != -1]\n",
    "            df_dt2 = df_only.loc[df_only['Has '+ d_type2] != -1]\n",
    "            probs = [[x,y,z] for x,y,z in zip(df_only['Integration Prob LUAD'], df_only['Integration Prob HLT'], df_only['Integration Prob LUSC'])]\n",
    "            probs = np.asarray(probs)\n",
    "            preds = df_only['Integration Pred'].values\n",
    "            real = df_only['Real'].values\n",
    "            real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "            \n",
    "            luad_s = len(np.where(real == 0)[0])\n",
    "            hlt_s = len(np.where(real == 1)[0])\n",
    "            lusc_s = len(np.where(real == 2)[0])\n",
    "\n",
    "            lengths[name]['luad'] += luad_s\n",
    "            lengths[name]['hlt'] += hlt_s\n",
    "            lengths[name]['lusc'] += lusc_s\n",
    "            acc = accuracy_score(real, preds)*100\n",
    "            f1 = f1_score(real, preds, average='weighted')*100\n",
    "            try:\n",
    "                auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "                aucs[name]['Integration'].append(auc)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "                \n",
    "                auprcs[name]['Integration'].append(np.mean(array))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            accs[name]['Integration'].append(acc)\n",
    "            f1_scores[name]['Integration'].append(f1)\n",
    "            \n",
    "            \n",
    "            # dtype1\n",
    "            probs = [[x,y,z] for x,y,z in zip(df_only[d_type1+' Prob LUAD'], df_only[d_type1+' Prob HLT'], df_only[d_type1+' Prob LUSC'])]\n",
    "            probs = np.asarray(probs)\n",
    "            preds = df_only[d_type1+' Pred'].values\n",
    "            real = df_only['Real'].values\n",
    "            real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "            acc = accuracy_score(real, preds)*100\n",
    "            f1 = f1_score(real, preds, average='weighted')*100\n",
    "            try:\n",
    "                auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "                aucs[name][d_type1+'Int'].append(auc)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "                \n",
    "                auprcs[name][d_type1+'Int'].append(np.mean(array))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            accs[name][d_type1+'Int'].append(acc)\n",
    "            f1_scores[name][d_type1+'Int'].append(f1)\n",
    "            \n",
    "            \n",
    "            # dtype2\n",
    "            probs = [[x,y,z] for x,y,z in zip(df_only[d_type2+' Prob LUAD'], df_only[d_type2+' Prob HLT'], df_only[d_type2+' Prob LUSC'])]\n",
    "            probs = np.asarray(probs)\n",
    "            preds = df_only[d_type2+' Pred'].values\n",
    "            real = df_only['Real'].values\n",
    "            real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "            acc = accuracy_score(real, preds)*100\n",
    "            f1 = f1_score(real, preds, average='weighted')*100\n",
    "            try:\n",
    "                auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "                aucs[name][d_type2+'Int'].append(auc)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "                \n",
    "                auprcs[name][d_type2+'Int'].append(np.mean(array))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            accs[name][d_type2+'Int'].append(acc)\n",
    "            f1_scores[name][d_type2+'Int'].append(f1)\n",
    "            \n",
    "    i += 1\n",
    "    j = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122a0ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 1\n",
    "\n",
    "for d_type1 in data_types[i:]:\n",
    "    for d_type2 in data_types[j:]:\n",
    "        print(5*'-')\n",
    "        name = d_type1 + '-' + d_type2\n",
    "        print('Number of samples {}'.format(lengths[name]))\n",
    "        print(name + ' ACC: {}+-{}'.format(np.mean(accs[name]['Integration']),np.std(accs[name]['Integration'])))\n",
    "        print(d_type1 + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type1+'Int']),np.std(accs[name][d_type1+'Int'])))\n",
    "        print(d_type2 + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type2+'Int']),np.std(accs[name][d_type2+'Int'])))\n",
    "        \n",
    "    i += 1\n",
    "    j = i + 1\n",
    "print(10*'-')\n",
    "\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "for d_type1 in data_types[i:]:\n",
    "    for d_type2 in data_types[j:]:\n",
    "        name = d_type1 + '-' + d_type2\n",
    "        print(5*'-')\n",
    "        print(name + ' F1: {}+-{}'.format(np.mean(f1_scores[name]['Integration']),np.std(f1_scores[name]['Integration'])))\n",
    "        print(d_type1 + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type1+'Int']),np.std(f1_scores[name][d_type1+'Int'])))\n",
    "        print(d_type2 + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type2+'Int']),np.std(f1_scores[name][d_type2+'Int'])))\n",
    "    i += 1\n",
    "    j = i + 1\n",
    "print(10*'-')\n",
    "\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "for d_type1 in data_types[i:]:\n",
    "    for d_type2 in data_types[j:]:\n",
    "        print(5*'-')\n",
    "        name = d_type1 + '-' + d_type2\n",
    "        print(name + ' AUC: {}+-{}'.format(np.mean(aucs[name]['Integration']),np.std(aucs[name]['Integration'])))\n",
    "        print(d_type1 + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type1+'Int']),np.std(aucs[name][d_type1+'Int'])))\n",
    "        print(d_type2 + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type2+'Int']),np.std(aucs[name][d_type2+'Int'])))\n",
    "    i += 1\n",
    "    j = i + 1\n",
    "\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "for d_type1 in data_types[i:]:\n",
    "    for d_type2 in data_types[j:]:\n",
    "        print(5*'-')\n",
    "        name = d_type1 + '-' + d_type2\n",
    "        print(name + ' AUPRC: {}+-{}'.format(np.mean(auprcs[name]['Integration']),np.std(auprcs[name]['Integration'])))\n",
    "        print(d_type1 + ' AUPRC: {}+-{}'.format(np.mean(auprcs[name][d_type1+'Int']),np.std(auprcs[name][d_type1+'Int'])))\n",
    "        print(d_type2 + ' AUPRC: {}+-{}'.format(np.mean(auprcs[name][d_type2+'Int']),np.std(auprcs[name][d_type2+'Int'])))\n",
    "    i += 1\n",
    "    j = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ad3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 1\n",
    "\n",
    "for d_type1 in data_types[i:]:\n",
    "    for d_type2 in data_types[j:]:\n",
    "        name = d_type1 + '-' + d_type2\n",
    "        print('{}: Number of samples {}'.format(name, lengths[name]))\n",
    "    i += 1\n",
    "    j = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a29e18",
   "metadata": {},
   "source": [
    "## check best integration three sources without NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d855227d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data_types = ['WSI', 'RNA', 'miRNA', 'CNV', 'DNA']\n",
    "data_types2 = copy.copy(data_types)\n",
    "data_types3 = copy.copy(data_types)\n",
    "\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "\n",
    "accs = {}\n",
    "f1_scores = {}\n",
    "aucs = {}\n",
    "auprcs = {}\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            name = d_type1 + '-' + d_type2 + '-' + d_type3\n",
    "            data_model = pd.read_excel('../result_files/three-sources-integration/data_integration_model_test_probs_'+name+'.xlsx',\n",
    "              sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "        \n",
    "            accs[name] = {}\n",
    "            f1_scores[name] = {}\n",
    "            aucs[name] = {}\n",
    "            auprcs[name] = {}\n",
    "                \n",
    "            accs[name]['Integration'] = []\n",
    "            accs[name][d_type1+'Int'] = []\n",
    "            accs[name][d_type2+'Int'] = []\n",
    "            accs[name][d_type3+'Int'] = []\n",
    "\n",
    "            f1_scores[name]['Integration'] = []\n",
    "            f1_scores[name][d_type1+'Int'] = []\n",
    "            f1_scores[name][d_type2+'Int'] = []\n",
    "            f1_scores[name][d_type3+'Int'] = []\n",
    "\n",
    "            aucs[name]['Integration'] = []\n",
    "            aucs[name][d_type1+'Int'] = []\n",
    "            aucs[name][d_type2+'Int'] = []\n",
    "            aucs[name][d_type3+'Int'] = []\n",
    "            \n",
    "            auprcs[name]['Integration'] = []\n",
    "            auprcs[name][d_type1+'Int'] = []\n",
    "            auprcs[name][d_type2+'Int'] = []\n",
    "            auprcs[name][d_type3+'Int'] = []\n",
    "            \n",
    "            lengths[name] = {'luad': 0, 'hlt': 0, 'lusc': 0}\n",
    "            for df_name, df in data_model.items():\n",
    "                # take those where the two sources has data\n",
    "                df_only = df.loc[(df['Has '+ d_type1] != -1) & (df['Has ' + d_type2] != -1) & (df['Has ' + d_type3] != -1)]\n",
    "                df_dt1 = df.loc[df['Has '+ d_type1] != -1]\n",
    "                df_dt2 = df.loc[df['Has '+ d_type2] != -1]\n",
    "                df_dt3 = df.loc[df['Has '+ d_type3] != -1]\n",
    "                \n",
    "                probs = [[x,y,z] for x,y,z in zip(df_only['Integration Prob LUAD'], df_only['Integration Prob HLT'], df_only['Integration Prob LUSC'])]\n",
    "                probs = np.asarray(probs)\n",
    "                preds = df_only['Integration Pred'].values\n",
    "                real = df_only['Real'].values\n",
    "                real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "                acc = accuracy_score(real, preds)*100\n",
    "                f1 = f1_score(real, preds, average='weighted')*100\n",
    "                try:\n",
    "                    auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "                    aucs[name]['Integration'].append(auc)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                    aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                    aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                    array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "                    auprcs[name]['Integration'].append(np.mean(array))\n",
    "                except:\n",
    "                    pass\n",
    "                aucs[name]['Integration'].append(auc)\n",
    "                accs[name]['Integration'].append(acc)\n",
    "                f1_scores[name]['Integration'].append(f1)\n",
    "                \n",
    "                luad_s = len(np.where(real == 0)[0])\n",
    "                hlt_s = len(np.where(real == 1)[0])\n",
    "                lusc_s = len(np.where(real == 2)[0])\n",
    "\n",
    "                lengths[name]['luad'] += luad_s\n",
    "                lengths[name]['hlt'] += hlt_s\n",
    "                lengths[name]['lusc'] += lusc_s\n",
    "            \n",
    "                # dtype1\n",
    "                probs = [[x,y,z] for x,y,z in zip(df_only[d_type1+' Prob LUAD'], df_only[d_type1+' Prob HLT'], df_only[d_type1+' Prob LUSC'])]\n",
    "                probs = np.asarray(probs)\n",
    "                preds = df_only[d_type1+' Pred'].values\n",
    "                real = df_only['Real'].values\n",
    "                real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "                acc = accuracy_score(real, preds)*100\n",
    "                f1 = f1_score(real, preds, average='weighted')*100\n",
    "                try:\n",
    "                    auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "                    aucs[name][d_type1+'Int'].append(auc)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                    aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                    aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                    array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "                    auprcs[name][d_type1+'Int'].append(np.mean(array))\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                accs[name][d_type1+'Int'].append(acc)\n",
    "                f1_scores[name][d_type1+'Int'].append(f1)\n",
    "\n",
    "\n",
    "                # dtype2\n",
    "                probs = [[x,y,z] for x,y,z in zip(df_only[d_type2+' Prob LUAD'], df_only[d_type2+' Prob HLT'], df_only[d_type2+' Prob LUSC'])]\n",
    "                probs = np.asarray(probs)\n",
    "                preds = df_only[d_type2+' Pred'].values\n",
    "                real = df_only['Real'].values\n",
    "                real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "                acc = accuracy_score(real, preds)*100\n",
    "                f1 = f1_score(real, preds, average='weighted')*100\n",
    "                try:\n",
    "                    auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "                    aucs[name][d_type2+'Int'].append(auc)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                    aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                    aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                    array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "                    auprcs[name][d_type2+'Int'].append(np.mean(array))\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                accs[name][d_type2+'Int'].append(acc)\n",
    "                f1_scores[name][d_type2+'Int'].append(f1)\n",
    "                \n",
    "                # dtype3\n",
    "                probs = [[x,y,z] for x,y,z in zip(df_only[d_type3+' Prob LUAD'], df_only[d_type3+' Prob HLT'], df_only[d_type3+' Prob LUSC'])]\n",
    "                probs = np.asarray(probs)\n",
    "                preds = df_only[d_type3+' Pred'].values\n",
    "                real = df_only['Real'].values\n",
    "                real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "                acc = accuracy_score(real, preds)*100\n",
    "                f1 = f1_score(real, preds, average='weighted')*100\n",
    "                try:\n",
    "                    auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "                    aucs[name][d_type3+'Int'].append(auc)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                    aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                    aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                    array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "                    auprcs[name][d_type3+'Int'].append(np.mean(array))\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                accs[name][d_type3+'Int'].append(acc)\n",
    "                f1_scores[name][d_type3+'Int'].append(f1)\n",
    "                \n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e8b02f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            print(5*'-')\n",
    "            name = d_type1 + '-' + d_type2 + '-' + d_type3\n",
    "            print(name + ' ACC: {}+-{}'.format(np.mean(accs[name]['Integration']),np.std(accs[name]['Integration'])))\n",
    "            print(d_type1 + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type1+'Int']),np.std(accs[name][d_type1+'Int'])))\n",
    "            print(d_type2 + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type2+'Int']),np.std(accs[name][d_type2+'Int'])))\n",
    "            print(d_type3 + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type3+'Int']),np.std(accs[name][d_type3+'Int'])))\n",
    "        \n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "\n",
    "print(10*'-')\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            print(5*'-')\n",
    "            name = d_type1 + '-' + d_type2 + '-' + d_type3\n",
    "            print(name + ' F1: {}+-{}'.format(np.mean(f1_scores[name]['Integration']),np.std(f1_scores[name]['Integration'])))\n",
    "            print(d_type1 + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type1+'Int']),np.std(f1_scores[name][d_type1+'Int'])))\n",
    "            print(d_type2 + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type2+'Int']),np.std(f1_scores[name][d_type2+'Int'])))\n",
    "            print(d_type3 + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type3+'Int']),np.std(f1_scores[name][d_type3+'Int'])))\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "\n",
    "print(10*'-')\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            print(5*'-')\n",
    "            name = d_type1 + '-' + d_type2 + '-' + d_type3\n",
    "            print(name + ' AUC: {}+-{}'.format(np.mean(aucs[name]['Integration']),np.std(aucs[name]['Integration'])))\n",
    "            print(d_type1 + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type1+'Int']),np.std(aucs[name][d_type1+'Int'])))\n",
    "            print(d_type2 + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type2+'Int']),np.std(aucs[name][d_type2+'Int'])))\n",
    "            print(d_type3 + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type3+'Int']),np.std(aucs[name][d_type3+'Int'])))\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "\n",
    "print(10*'-')\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            print(5*'-')\n",
    "            name = d_type1 + '-' + d_type2 + '-' + d_type3\n",
    "            print(name + ' AUPRC: {}+-{}'.format(np.mean(auprcs[name]['Integration']),np.std(auprcs[name]['Integration'])))\n",
    "            print(d_type1 + ' AUPRC: {}+-{}'.format(np.mean(auprcs[name][d_type1+'Int']),np.std(auprcs[name][d_type1+'Int'])))\n",
    "            print(d_type2 + ' AUPRC: {}+-{}'.format(np.mean(auprcs[name][d_type2+'Int']),np.std(auprcs[name][d_type2+'Int'])))\n",
    "            print(d_type3 + ' AUPRC: {}+-{}'.format(np.mean(auprcs[name][d_type3+'Int']),np.std(auprcs[name][d_type3+'Int'])))\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            \n",
    "            name = d_type1 + '-' + d_type2 + '-' + d_type3\n",
    "            print('{}: Number of samples {}'.format(name, lengths[name]))\n",
    "        \n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281eda5d",
   "metadata": {},
   "source": [
    "## check best integration four sources without NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb2f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = ['WSI', 'RNA', 'miRNA', 'CNV', 'DNA']\n",
    "\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "z = 3\n",
    "\n",
    "accs = {}\n",
    "f1_scores = {}\n",
    "aucs = {}\n",
    "auprcs = {}\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            for d_type4 in data_types[z:]:\n",
    "                name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "                data_model = pd.read_excel('../result_files/four-sources-integration/data_integration_model_test_probs_'+name+'.xlsx',\n",
    "                  sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "        \n",
    "                accs[name] = {}\n",
    "                f1_scores[name] = {}\n",
    "                aucs[name] = {}\n",
    "                auprcs[name] = {}\n",
    "                \n",
    "                accs[name]['Integration'] = []\n",
    "                accs[name][d_type1+'Int'] = []\n",
    "                accs[name][d_type2+'Int'] = []\n",
    "                accs[name][d_type3+'Int'] = []\n",
    "                accs[name][d_type4+'Int'] = []\n",
    "                \n",
    "                f1_scores[name]['Integration'] = []\n",
    "                f1_scores[name][d_type1+'Int'] = []\n",
    "                f1_scores[name][d_type2+'Int'] = []\n",
    "                f1_scores[name][d_type3+'Int'] = []\n",
    "                f1_scores[name][d_type4+'Int'] = []\n",
    "                \n",
    "                aucs[name]['Integration'] = []\n",
    "                aucs[name][d_type1+'Int'] = []\n",
    "                aucs[name][d_type2+'Int'] = []\n",
    "                aucs[name][d_type3+'Int'] = []\n",
    "                aucs[name][d_type4+'Int'] = []\n",
    "                \n",
    "                auprcs[name]['Integration'] = []\n",
    "                auprcs[name][d_type1+'Int'] = []\n",
    "                auprcs[name][d_type2+'Int'] = []\n",
    "                auprcs[name][d_type3+'Int'] = []\n",
    "                auprcs[name][d_type4+'Int'] = []\n",
    "                \n",
    "                lengths[name] = {'luad': 0, 'hlt': 0, 'lusc': 0}\n",
    "                \n",
    "                for df_name, df in data_model.items():\n",
    "                    # take those where the four sources has data\n",
    "                    df_only = df.loc[(df['Has '+ d_type1] != -1) & (df['Has ' + d_type2] != -1) & (df['Has ' + d_type3] != -1) & (df['Has ' + d_type4] != -1)]\n",
    "                    df_dt1 = df.loc[df['Has '+ d_type1] != -1]\n",
    "                    df_dt2 = df.loc[df['Has '+ d_type2] != -1]\n",
    "                    df_dt3 = df.loc[df['Has '+ d_type3] != -1]\n",
    "                    df_dt4 = df.loc[df['Has '+ d_type4] != -1]\n",
    "                    \n",
    "                    probs = [[x,y,z] for x,y,z in zip(df_only['Integration Prob LUAD'], df_only['Integration Prob HLT'], df_only['Integration Prob LUSC'])]\n",
    "                    probs = np.asarray(probs)\n",
    "                    preds = df_only['Integration Pred'].values\n",
    "                    real = df_only['Real'].values\n",
    "                    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "                    acc = accuracy_score(real, preds)*100\n",
    "                    f1 = f1_score(real, preds, average='weighted')*100\n",
    "                    try:\n",
    "                        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "                        aucs[name]['Integration'].append(auc)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "                        auprcs[name]['Integration'].append(np.mean(array))\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                    aucs[name]['Integration'].append(auc)\n",
    "                    accs[name]['Integration'].append(acc)\n",
    "                    f1_scores[name]['Integration'].append(f1)\n",
    "                    \n",
    "                    luad_s = len(np.where(real == 0)[0])\n",
    "                    hlt_s = len(np.where(real == 1)[0])\n",
    "                    lusc_s = len(np.where(real == 2)[0])\n",
    "\n",
    "                    lengths[name]['luad'] += luad_s\n",
    "                    lengths[name]['hlt'] += hlt_s\n",
    "                    lengths[name]['lusc'] += lusc_s\n",
    "                    \n",
    "                    # dtype1\n",
    "                    probs = [[x,y,z] for x,y,z in zip(df_only[d_type1+' Prob LUAD'], df_only[d_type1+' Prob HLT'], df_only[d_type1+' Prob LUSC'])]\n",
    "                    probs = np.asarray(probs)\n",
    "                    preds = df_only[d_type1+' Pred'].values\n",
    "                    real = df_only['Real'].values\n",
    "                    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "                    acc = accuracy_score(real, preds)*100\n",
    "                    f1 = f1_score(real, preds, average='weighted')*100\n",
    "                    try:\n",
    "                        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "                        aucs[name][d_type1+'Int'].append(auc)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "                        auprcs[name][d_type1+'Int'].append(np.mean(array))\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    accs[name][d_type1+'Int'].append(acc)\n",
    "                    f1_scores[name][d_type1+'Int'].append(f1)\n",
    "\n",
    "\n",
    "                    # dtype2\n",
    "                    probs = [[x,y,z] for x,y,z in zip(df_only[d_type2+' Prob LUAD'], df_only[d_type2+' Prob HLT'], df_only[d_type2+' Prob LUSC'])]\n",
    "                    probs = np.asarray(probs)\n",
    "                    preds = df_only[d_type2+' Pred'].values\n",
    "                    real = df_only['Real'].values\n",
    "                    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "                    acc = accuracy_score(real, preds)*100\n",
    "                    f1 = f1_score(real, preds, average='weighted')*100\n",
    "                    try:\n",
    "                        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "                        aucs[name][d_type2+'Int'].append(auc)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "                        auprcs[name][d_type2+'Int'].append(np.mean(array))\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    accs[name][d_type2+'Int'].append(acc)\n",
    "                    f1_scores[name][d_type2+'Int'].append(f1)\n",
    "\n",
    "                    # dtype3\n",
    "                    probs = [[x,y,z] for x,y,z in zip(df_only[d_type3+' Prob LUAD'], df_only[d_type3+' Prob HLT'], df_only[d_type3+' Prob LUSC'])]\n",
    "                    probs = np.asarray(probs)\n",
    "                    preds = df_only[d_type3+' Pred'].values\n",
    "                    real = df_only['Real'].values\n",
    "                    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "                    acc = accuracy_score(real, preds)*100\n",
    "                    f1 = f1_score(real, preds, average='weighted')*100\n",
    "                    try:\n",
    "                        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "                        aucs[name][d_type3+'Int'].append(auc)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "                        auprcs[name][d_type3+'Int'].append(np.mean(array))\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    \n",
    "                    accs[name][d_type3+'Int'].append(acc)\n",
    "                    f1_scores[name][d_type3+'Int'].append(f1)\n",
    "                    \n",
    "                    # dtype4\n",
    "                    probs = [[x,y,z] for x,y,z in zip(df_only[d_type4+' Prob LUAD'], df_only[d_type4+' Prob HLT'], df_only[d_type4+' Prob LUSC'])]\n",
    "                    probs = np.asarray(probs)\n",
    "                    preds = df_only[d_type4+' Pred'].values\n",
    "                    real = df_only['Real'].values\n",
    "                    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "                    acc = accuracy_score(real, preds)*100\n",
    "                    f1 = f1_score(real, preds, average='weighted')*100\n",
    "                    try:\n",
    "                        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "                        aucs[name][d_type4+'Int'].append(auc)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "                        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "                        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "                        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "                        auprcs[name][d_type4+'Int'].append(np.mean(array))\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    accs[name][d_type4+'Int'].append(acc)\n",
    "                    f1_scores[name][d_type4+'Int'].append(f1)\n",
    "            z += 1\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "    z = j + 1\n",
    "\n",
    "d_type1 = 'WSI'\n",
    "d_type2 = 'miRNA'\n",
    "d_type3 = 'CNV'\n",
    "d_type4 = 'DNA'\n",
    "\n",
    "name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "data_model = pd.read_excel('../result_files/four-sources-integration/data_integration_model_test_probs_'+name+'.xlsx',\n",
    "sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "\n",
    "accs[name] = {}\n",
    "f1_scores[name] = {}\n",
    "aucs[name] = {}\n",
    "auprcs[name] = {}\n",
    "\n",
    "accs[name]['Integration'] = []\n",
    "accs[name][d_type1+'Int'] = []\n",
    "accs[name][d_type2+'Int'] = []\n",
    "accs[name][d_type3+'Int'] = []\n",
    "accs[name][d_type4+'Int'] = []\n",
    "\n",
    "f1_scores[name]['Integration'] = []\n",
    "f1_scores[name][d_type1+'Int'] = []\n",
    "f1_scores[name][d_type2+'Int'] = []\n",
    "f1_scores[name][d_type3+'Int'] = []\n",
    "f1_scores[name][d_type4+'Int'] = []\n",
    "\n",
    "aucs[name]['Integration'] = []\n",
    "aucs[name][d_type1+'Int'] = []\n",
    "aucs[name][d_type2+'Int'] = []\n",
    "aucs[name][d_type3+'Int'] = []\n",
    "aucs[name][d_type4+'Int'] = []\n",
    "\n",
    "auprcs[name]['Integration'] = []\n",
    "auprcs[name][d_type1+'Int'] = []\n",
    "auprcs[name][d_type2+'Int'] = []\n",
    "auprcs[name][d_type3+'Int'] = []\n",
    "auprcs[name][d_type4+'Int'] = []\n",
    "\n",
    "lengths[name] = {'luad': 0, 'hlt': 0, 'lusc': 0}\n",
    "\n",
    "for df_name, df in data_model.items():\n",
    "    # take those where the four sources has data\n",
    "    df_only = df.loc[(df['Has '+ d_type1] != -1) & (df['Has ' + d_type2] != -1) & (df['Has ' + d_type3] != -1) & (df['Has ' + d_type4] != -1)]\n",
    "    df_dt1 = df.loc[df['Has '+ d_type1] != -1]\n",
    "    df_dt2 = df.loc[df['Has '+ d_type2] != -1]\n",
    "    df_dt3 = df.loc[df['Has '+ d_type3] != -1]\n",
    "    df_dt4 = df.loc[df['Has '+ d_type4] != -1]\n",
    "\n",
    "    probs = [[x,y,z] for x,y,z in zip(df_only['Integration Prob LUAD'], df_only['Integration Prob HLT'], df_only['Integration Prob LUSC'])]\n",
    "    probs = np.asarray(probs)\n",
    "    preds = df_only['Integration Pred'].values\n",
    "    real = df_only['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "    acc = accuracy_score(real, preds)*100\n",
    "    f1 = f1_score(real, preds, average='weighted')*100\n",
    "    try:\n",
    "        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "        aucs[name]['Integration'].append(auc)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "        auprcs[name]['Integration'].append(np.mean(array))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    aucs[name]['Integration'].append(auc)\n",
    "    accs[name]['Integration'].append(acc)\n",
    "    f1_scores[name]['Integration'].append(f1)\n",
    "\n",
    "    luad_s = len(np.where(real == 0)[0])\n",
    "    hlt_s = len(np.where(real == 1)[0])\n",
    "    lusc_s = len(np.where(real == 2)[0])\n",
    "\n",
    "    lengths[name]['luad'] += luad_s\n",
    "    lengths[name]['hlt'] += hlt_s\n",
    "    lengths[name]['lusc'] += lusc_s\n",
    "\n",
    "    # dtype1\n",
    "    probs = [[x,y,z] for x,y,z in zip(df_only[d_type1+' Prob LUAD'], df_only[d_type1+' Prob HLT'], df_only[d_type1+' Prob LUSC'])]\n",
    "    probs = np.asarray(probs)\n",
    "    preds = df_only[d_type1+' Pred'].values\n",
    "    real = df_only['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "    acc = accuracy_score(real, preds)*100\n",
    "    f1 = f1_score(real, preds, average='weighted')*100\n",
    "    try:\n",
    "        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "        aucs[name][d_type1+'Int'].append(auc)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "        auprcs[name][d_type1+'Int'].append(np.mean(array))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    accs[name][d_type1+'Int'].append(acc)\n",
    "    f1_scores[name][d_type1+'Int'].append(f1)\n",
    "\n",
    "\n",
    "    # dtype2\n",
    "    probs = [[x,y,z] for x,y,z in zip(df_only[d_type2+' Prob LUAD'], df_only[d_type2+' Prob HLT'], df_only[d_type2+' Prob LUSC'])]\n",
    "    probs = np.asarray(probs)\n",
    "    preds = df_only[d_type2+' Pred'].values\n",
    "    real = df_only['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "    acc = accuracy_score(real, preds)*100\n",
    "    f1 = f1_score(real, preds, average='weighted')*100\n",
    "    try:\n",
    "        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "        aucs[name][d_type2+'Int'].append(auc)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "        auprcs[name][d_type2+'Int'].append(np.mean(array))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    accs[name][d_type2+'Int'].append(acc)\n",
    "    f1_scores[name][d_type2+'Int'].append(f1)\n",
    "\n",
    "    # dtype3\n",
    "    probs = [[x,y,z] for x,y,z in zip(df_only[d_type3+' Prob LUAD'], df_only[d_type3+' Prob HLT'], df_only[d_type3+' Prob LUSC'])]\n",
    "    probs = np.asarray(probs)\n",
    "    preds = df_only[d_type3+' Pred'].values\n",
    "    real = df_only['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "    acc = accuracy_score(real, preds)*100\n",
    "    f1 = f1_score(real, preds, average='weighted')*100\n",
    "    try:\n",
    "        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "        aucs[name][d_type3+'Int'].append(auc)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "        auprcs[name][d_type3+'Int'].append(np.mean(array))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    accs[name][d_type3+'Int'].append(acc)\n",
    "    f1_scores[name][d_type3+'Int'].append(f1)\n",
    "\n",
    "    # dtype4\n",
    "    probs = [[x,y,z] for x,y,z in zip(df_only[d_type4+' Prob LUAD'], df_only[d_type4+' Prob HLT'], df_only[d_type4+' Prob LUSC'])]\n",
    "    probs = np.asarray(probs)\n",
    "    preds = df_only[d_type4+' Pred'].values\n",
    "    real = df_only['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "    acc = accuracy_score(real, preds)*100\n",
    "    f1 = f1_score(real, preds, average='weighted')*100\n",
    "    try:\n",
    "        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "        aucs[name][d_type4+'Int'].append(auc)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "        auprcs[name][d_type4+'Int'].append(np.mean(array))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    accs[name][d_type4+'Int'].append(acc)\n",
    "    f1_scores[name][d_type4+'Int'].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c31da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "z = 3\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            for d_type4 in data_types[z:]:\n",
    "                print(5*'-')\n",
    "                name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "                print(name + ' ACC: {}+-{}'.format(np.mean(accs[name]['Integration']),np.std(accs[name]['Integration'])))\n",
    "                print(d_type1 + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type1+'Int']),np.std(accs[name][d_type1+'Int'])))\n",
    "                print(d_type2 + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type2+'Int']),np.std(accs[name][d_type2+'Int'])))\n",
    "                print(d_type3 + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type3+'Int']),np.std(accs[name][d_type3+'Int'])))\n",
    "                print(d_type4 + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type4+'Int']),np.std(accs[name][d_type4+'Int'])))\n",
    "            z += 1\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "    z = j + 1\n",
    "\n",
    "print(10*'-')\n",
    "\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "z = 3\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            for d_type4 in data_types[z:]:\n",
    "                print(5*'-')\n",
    "                name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "                print(name + ' F1: {}+-{}'.format(np.mean(f1_scores[name]['Integration']),np.std(f1_scores[name]['Integration'])))\n",
    "                print(d_type1 + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type1+'Int']),np.std(f1_scores[name][d_type1+'Int'])))\n",
    "                print(d_type2 + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type2+'Int']),np.std(f1_scores[name][d_type2+'Int'])))\n",
    "                print(d_type3 + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type3+'Int']),np.std(f1_scores[name][d_type3+'Int'])))\n",
    "                print(d_type4 + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type4+'Int']),np.std(f1_scores[name][d_type4+'Int'])))\n",
    "            z += 1\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "    z = j + 1\n",
    "\n",
    "print(10*'-')\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "z = 3\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            for d_type4 in data_types[z:]:\n",
    "                print(5*'-')\n",
    "                name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "                print(name + ' AUC: {}+-{}'.format(np.mean(aucs[name]['Integration']),np.std(aucs[name]['Integration'])))\n",
    "                print(d_type1 + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type1+'Int']),np.std(aucs[name][d_type1+'Int'])))\n",
    "                print(d_type2 + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type2+'Int']),np.std(aucs[name][d_type2+'Int'])))\n",
    "                print(d_type3 + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type3+'Int']),np.std(aucs[name][d_type3+'Int'])))\n",
    "                print(d_type4 + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type4+'Int']),np.std(aucs[name][d_type4+'Int'])))\n",
    "            z += 1\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "    z = j + 1\n",
    "\n",
    "print(10*'-')\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "z = 3\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            for d_type4 in data_types[z:]:\n",
    "                print(5*'-')\n",
    "                name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "                print(name + ' AUPRC: {}+-{}'.format(np.mean(auprcs[name]['Integration']),np.std(auprcs[name]['Integration'])))\n",
    "                print(d_type1 + ' AUPRC: {}+-{}'.format(np.mean(auprcs[name][d_type1+'Int']),np.std(auprcs[name][d_type1+'Int'])))\n",
    "                print(d_type2 + ' AUPRC: {}+-{}'.format(np.mean(auprcs[name][d_type2+'Int']),np.std(auprcs[name][d_type2+'Int'])))\n",
    "                print(d_type3 + ' AUPRC: {}+-{}'.format(np.mean(auprcs[name][d_type3+'Int']),np.std(auprcs[name][d_type3+'Int'])))\n",
    "                print(d_type4 + ' AUPRC: {}+-{}'.format(np.mean(auprcs[name][d_type4+'Int']),np.std(auprcs[name][d_type4+'Int'])))\n",
    "            z += 1\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "    z = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78febc0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "z = 3\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            for d_type4 in data_types[z:]:\n",
    "                \n",
    "                name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "                print('{}: Number of samples {}'.format(name, lengths[name]))\n",
    "            z += 1\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "    z = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4096cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "print(name + ' ACC: {}+-{}'.format(np.mean(accs[name]['Integration']),np.std(accs[name]['Integration'])))\n",
    "print(d_type1 + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type1+'Int']),np.std(accs[name][d_type1+'Int'])))\n",
    "print(d_type2 + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type2+'Int']),np.std(accs[name][d_type2+'Int'])))\n",
    "print(d_type3 + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type3+'Int']),np.std(accs[name][d_type3+'Int'])))\n",
    "print(d_type4 + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type4+'Int']),np.std(accs[name][d_type4+'Int'])))\n",
    "\n",
    "print(10*'-')\n",
    "print(name + ' F1: {}+-{}'.format(np.mean(f1_scores[name]['Integration']),np.std(f1_scores[name]['Integration'])))\n",
    "print(d_type1 + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type1+'Int']),np.std(f1_scores[name][d_type1+'Int'])))\n",
    "print(d_type2 + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type2+'Int']),np.std(f1_scores[name][d_type2+'Int'])))\n",
    "print(d_type3 + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type3+'Int']),np.std(f1_scores[name][d_type3+'Int'])))\n",
    "print(d_type4 + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type4+'Int']),np.std(f1_scores[name][d_type4+'Int'])))\n",
    "\n",
    "print(10*'-')\n",
    "\n",
    "print(name + ' AUC: {}+-{}'.format(np.mean(aucs[name]['Integration']),np.std(aucs[name]['Integration'])))\n",
    "print(d_type1 + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type1+'Int']),np.std(aucs[name][d_type1+'Int'])))\n",
    "print(d_type2 + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type2+'Int']),np.std(aucs[name][d_type2+'Int'])))\n",
    "print(d_type3 + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type3+'Int']),np.std(aucs[name][d_type3+'Int'])))\n",
    "print(d_type4 + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type4+'Int']),np.std(aucs[name][d_type4+'Int'])))\n",
    "\n",
    "print(10*'-')\n",
    "\n",
    "print(name + ' AUPRC: {}+-{}'.format(np.mean(auprcs[name]['Integration']),np.std(auprcs[name]['Integration'])))\n",
    "print(d_type1 + ' AUPRC: {}+-{}'.format(np.mean(auprcs[name][d_type1+'Int']),np.std(auprcs[name][d_type1+'Int'])))\n",
    "print(d_type2 + ' AUPRC: {}+-{}'.format(np.mean(auprcs[name][d_type2+'Int']),np.std(auprcs[name][d_type2+'Int'])))\n",
    "print(d_type3 + ' AUPRC: {}+-{}'.format(np.mean(auprcs[name][d_type3+'Int']),np.std(auprcs[name][d_type3+'Int'])))\n",
    "print(d_type4 + ' AUPRC: {}+-{}'.format(np.mean(auprcs[name][d_type4+'Int']),np.std(auprcs[name][d_type4+'Int'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab42768",
   "metadata": {},
   "source": [
    "### Ommit healthy class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = ['WSI', 'RNA', 'miRNA', 'CNV', 'DNA']\n",
    "\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "z = 3\n",
    "\n",
    "accs = {}\n",
    "f1_scores = {}\n",
    "aucs = {}\n",
    "lengths = {}\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            for d_type4 in data_types[z:]:\n",
    "                name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "                data_model = pd.read_excel('results_SGD/four-sources-integration/data_integration_model_test_probs_'+name+'.xlsx',\n",
    "                  sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "        \n",
    "                accs[name] = {}\n",
    "                f1_scores[name] = {}\n",
    "                aucs[name] = {}\n",
    "                \n",
    "                accs[name]['Integration'] = []\n",
    "                accs[name][d_type1+'Int'] = []\n",
    "                accs[name][d_type2+'Int'] = []\n",
    "                accs[name][d_type3+'Int'] = []\n",
    "                accs[name][d_type4+'Int'] = []\n",
    "                \n",
    "                f1_scores[name]['Integration'] = []\n",
    "                f1_scores[name][d_type1+'Int'] = []\n",
    "                f1_scores[name][d_type2+'Int'] = []\n",
    "                f1_scores[name][d_type3+'Int'] = []\n",
    "                f1_scores[name][d_type4+'Int'] = []\n",
    "                \n",
    "                aucs[name]['Integration'] = []\n",
    "                aucs[name][d_type1+'Int'] = []\n",
    "                aucs[name][d_type2+'Int'] = []\n",
    "                aucs[name][d_type3+'Int'] = []\n",
    "                aucs[name][d_type4+'Int'] = []\n",
    "                \n",
    "                lengths[name] = {'luad': 0, 'lusc': 0}\n",
    "                \n",
    "                for df_name, df in data_model.items():\n",
    "                    # take those where the four sources has data\n",
    "                    df_only = df.loc[(df['Has '+ d_type1] != -1) & (df['Has ' + d_type2] != -1) & (df['Has ' + d_type3] != -1) & (df['Has ' + d_type4] != -1)]\n",
    "                    df_dt1 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = ['WSI', 'RNA', 'miRNA', 'CNV', 'DNA']\n",
    "\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "z = 3\n",
    "\n",
    "accs = {}\n",
    "f1_scores = {}\n",
    "aucs = {}\n",
    "lengths = {}\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            for d_type4 in data_types[z:]:\n",
    "                name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "                data_model = pd.read_excel('results_SGD/four-sources-integration/data_integration_model_test_probs_'+name+'.xlsx',\n",
    "                  sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "        \n",
    "                accs[name] = {}\n",
    "                f1_scores[name] = {}\n",
    "                aucs[name] = {}\n",
    "                \n",
    "                accs[name]['Integration'] = []\n",
    "                accs[name][d_type1+'Int'] = []\n",
    "                accs[name][d_type2+'Int'] = []\n",
    "                accs[name][d_type3+'Int'] = []\n",
    "                accs[name][d_type4+'Int'] = []\n",
    "                \n",
    "                f1_scores[name]['Integration'] = []\n",
    "                f1_scores[name][d_type1+'Int'] = []\n",
    "                f1_scores[name][d_type2+'Int'] = []\n",
    "                f1_scores[name][d_type3+'Int'] = []\n",
    "                f1_scores[name][d_type4+'Int'] = []\n",
    "                \n",
    "                aucs[name]['Integration'] = []\n",
    "                aucs[name][d_type1+'Int'] = []\n",
    "                aucs[name][d_type2+'Int'] = []\n",
    "                aucs[name][d_type3+'Int'] = []\n",
    "                aucs[name][d_type4+'Int'] = []\n",
    "                \n",
    "                lengths[name] = {'luad': 0, 'lusc': 0}\n",
    "                \n",
    "                for df_name, df in data_model.items():\n",
    "                    # take those where the four sources has data\n",
    "                    df_only = df.loc[(df['Has '+ d_type1] != -1) & (df['Has ' + d_type2] != -1) & (df['Has ' + d_type3] != -1) & (df['Has ' + d_type4] != -1)]\n",
    "                    df_dt1 = df.loc[df['Has '+ d_type1] != -1]\n",
    "                    df_dt2 = df.loc[df['Has '+ d_type2] != -1]\n",
    "                    df_dt3 = df.loc[df['Has '+ d_type3] != -1]\n",
    "                    df_dt4 = df.loc[df['Has '+ d_type4] != -1]\n",
    "                    \n",
    "                    df_only = df_only[df_only['Real'] != 1]\n",
    "                    df_dt1 = df_dt1[df_dt1['Real'] != 1]\n",
    "                    df_dt2 = df_dt2[df_dt2['Real'] != 1]\n",
    "                    df_dt3 = df_dt3[df_dt3['Real'] != 1]\n",
    "                    df_dt4 = df_dt4[df_dt4['Real'] != 1]\n",
    "                    \n",
    "                    probs = [[x,y] for x,y in zip(df_only['Integration Prob LUAD'], df_only['Integration Prob LUSC'])]\n",
    "                    preds = df_only['Integration Pred'].values\n",
    "                    real = df_only['Real'].values\n",
    "                    real = np.where(real == 2, 1, 0)\n",
    "                    acc = accuracy_score(real, preds)*100\n",
    "                    f1 = f1_score(real, preds, average='weighted')*100\n",
    "                    \n",
    "                    auc = roc_auc_score(real, np.asarray(probs)[:,1])\n",
    "                    aucs[name]['Integration'].append(auc)\n",
    "                    \n",
    "                    aucs[name]['Integration'].append(auc)\n",
    "                    accs[name]['Integration'].append(acc)\n",
    "                    f1_scores[name]['Integration'].append(f1)\n",
    "                    \n",
    "                    luad_s = len(np.where(real == 0)[0])\n",
    "                    lusc_s = len(np.where(real == 2)[0])\n",
    "\n",
    "                    lengths[name]['luad'] += luad_s\n",
    "                    lengths[name]['lusc'] += lusc_s\n",
    "                    \n",
    "                    # dtype1\n",
    "                    probs = [[x,y] for x,y in zip(df_only[d_type1+' Prob LUAD'], df_only[d_type1+' Prob LUSC'])]\n",
    "                    preds = df_only[d_type1+' Pred'].values\n",
    "                    real = df_only['Real'].values\n",
    "                    real = np.where(real == 2, 1, 0)\n",
    "                    acc = accuracy_score(real, preds)*100\n",
    "                    f1 = f1_score(real, preds, average='weighted')*100\n",
    "                    try:\n",
    "                        auc = roc_auc_score(real, np.asarray(probs)[:,1])\n",
    "                        aucs[name][d_type1+'Int'].append(auc)\n",
    "                    except:\n",
    "                        pass\n",
    "                    accs[name][d_type1+'Int'].append(acc)\n",
    "                    f1_scores[name][d_type1+'Int'].append(f1)\n",
    "\n",
    "\n",
    "                    # dtype2\n",
    "                    probs = [[x,y] for x,y in zip(df_only[d_type2+' Prob LUAD'], df_only[d_type2+' Prob LUSC'])]\n",
    "                    preds = df_only[d_type2+' Pred'].values\n",
    "                    real = df_only['Real'].values\n",
    "                    real = np.where(real == 2, 1, 0)\n",
    "                    acc = accuracy_score(real, preds)*100\n",
    "                    f1 = f1_score(real, preds, average='weighted')*100\n",
    "                    try:\n",
    "                        auc = roc_auc_score(real, np.asarray(probs)[:,1])\n",
    "                        aucs[name][d_type2+'Int'].append(auc)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    accs[name][d_type2+'Int'].append(acc)\n",
    "                    f1_scores[name][d_type2+'Int'].append(f1)\n",
    "\n",
    "                    # dtype3\n",
    "                    probs = [[x,y] for x,y in zip(df_only[d_type3+' Prob LUAD'], df_only[d_type3+' Prob LUSC'])]\n",
    "                    preds = df_only[d_type3+' Pred'].values\n",
    "                    real = df_only['Real'].values\n",
    "                    real = np.where(real == 2, 1, 0)\n",
    "                    acc = accuracy_score(real, preds)*100\n",
    "                    f1 = f1_score(real, preds, average='weighted')*100\n",
    "                    try:\n",
    "                        auc = roc_auc_score(real, np.asarray(probs)[:,1])\n",
    "                        aucs[name][d_type3+'Int'].append(auc)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    accs[name][d_type3+'Int'].append(acc)\n",
    "                    f1_scores[name][d_type3+'Int'].append(f1)\n",
    "                    \n",
    "                    # dtype4\n",
    "                    probs = [[x,y] for x,y in zip(df_only[d_type4+' Prob LUAD'], df_only[d_type4+' Prob LUSC'])]\n",
    "                    preds = df_only[d_type4+' Pred'].values\n",
    "                    real = df_only['Real'].values\n",
    "                    real = np.where(real == 2, 1, 0)\n",
    "                    acc = accuracy_score(real, preds)*100\n",
    "                    f1 = f1_score(real, preds, average='weighted')*100\n",
    "                    try:\n",
    "                        auc = roc_auc_score(real, np.asarray(probs)[:,1])\n",
    "                        aucs[name][d_type4+'Int'].append(auc)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    accs[name][d_type4+'Int'].append(acc)\n",
    "                    f1_scores[name][d_type4+'Int'].append(f1)\n",
    "            z += 1\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "    z = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2fc9e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "z = 3\n",
    "\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            for d_type4 in data_types[z:]:\n",
    "                print(5*'-')\n",
    "                name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "                print(name + ' ACC: {}+-{}'.format(np.mean(accs[name]['Integration']),np.std(accs[name]['Integration'])))\n",
    "                print(d_type1 + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type1+'Int']),np.std(accs[name][d_type1+'Int'])))\n",
    "                print(d_type2 + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type2+'Int']),np.std(accs[name][d_type2+'Int'])))\n",
    "                print(d_type3 + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type3+'Int']),np.std(accs[name][d_type3+'Int'])))\n",
    "                print(d_type4 + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type4+'Int']),np.std(accs[name][d_type4+'Int'])))\n",
    "            z += 1\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "    z = j + 1\n",
    "\n",
    "print(10*'-')\n",
    "\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "z = 3\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            for d_type4 in data_types[z:]:\n",
    "                print(5*'-')\n",
    "                name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "                print(name + ' F1: {}+-{}'.format(np.mean(f1_scores[name]['Integration']),np.std(f1_scores[name]['Integration'])))\n",
    "                print(d_type1 + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type1+'Int']),np.std(f1_scores[name][d_type1+'Int'])))\n",
    "                print(d_type2 + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type2+'Int']),np.std(f1_scores[name][d_type2+'Int'])))\n",
    "                print(d_type3 + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type3+'Int']),np.std(f1_scores[name][d_type3+'Int'])))\n",
    "                print(d_type4 + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type4+'Int']),np.std(f1_scores[name][d_type4+'Int'])))\n",
    "            z += 1\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "    z = j + 1\n",
    "\n",
    "print(10*'-')\n",
    "k = 0\n",
    "i = 1\n",
    "j = 2\n",
    "z = 3\n",
    "for d_type1 in data_types[k:]:\n",
    "    for d_type2 in data_types[i:]:\n",
    "        for d_type3 in data_types[j:]:\n",
    "            for d_type4 in data_types[z:]:\n",
    "                print(5*'-')\n",
    "                name = d_type1 + '-' + d_type2 + '-' + d_type3 + '-' + d_type4\n",
    "                print(name + ' AUC: {}+-{}'.format(np.mean(aucs[name]['Integration']),np.std(aucs[name]['Integration'])))\n",
    "                print(d_type1 + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type1+'Int']),np.std(aucs[name][d_type1+'Int'])))\n",
    "                print(d_type2 + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type2+'Int']),np.std(aucs[name][d_type2+'Int'])))\n",
    "                print(d_type3 + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type3+'Int']),np.std(aucs[name][d_type3+'Int'])))\n",
    "                print(d_type4 + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type4+'Int']),np.std(aucs[name][d_type4+'Int'])))\n",
    "            z += 1\n",
    "        j += 1\n",
    "    k += 1    \n",
    "    i = k + 1\n",
    "    j = i + 1\n",
    "    z = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a354f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706bace7",
   "metadata": {},
   "source": [
    "## check best integration five sources without NANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169af8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "data_model = pd.read_excel('../result_files/data_integration_model_test_probs_SGD-all_sources.xlsx',\n",
    "              sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "\n",
    "accs = {\n",
    "    'WSI': [],\n",
    "    'RNA': [],\n",
    "    'miRNA': [],\n",
    "    'CNV': [],\n",
    "    'DNA': [],\n",
    "    'Integration': []\n",
    "}\n",
    "\n",
    "f1_scores = {\n",
    "    'WSI': [],\n",
    "    'RNA': [],\n",
    "    'miRNA': [],\n",
    "    'CNV': [],\n",
    "    'DNA': [],\n",
    "    'Integration': []\n",
    "}\n",
    "\n",
    "aucs = {\n",
    "    'WSI': [],\n",
    "    'RNA': [],\n",
    "    'miRNA': [],\n",
    "    'CNV': [],\n",
    "    'DNA': [],\n",
    "    'Integration': []\n",
    "}\n",
    "\n",
    "auprcs = {\n",
    "    'WSI': [],\n",
    "    'RNA': [],\n",
    "    'miRNA': [],\n",
    "    'CNV': [],\n",
    "    'DNA': [],\n",
    "    'Integration': []\n",
    "}\n",
    "\n",
    "sizes = 0\n",
    "lengths['all'] = {'luad': 0, 'hlt': 0, 'lusc': 0}\n",
    "for df_name, df in data_model.items():\n",
    "    \n",
    "    df_only = df.loc[(df['Has WSI'] != -1) & (df['Has RNA'] != -1) & (df['Has miRNA'] != -1) & (df['Has CNV'] != -1) & (df['Has DNA'] != -1)]\n",
    "    print(df_only.shape[0])\n",
    "    sizes += df_only.shape[0]\n",
    "    \n",
    "    luad_s = len(np.where(real == 0)[0])\n",
    "    hlt_s = len(np.where(real == 1)[0])\n",
    "    lusc_s = len(np.where(real == 2)[0])\n",
    "\n",
    "    lengths['all']['luad'] += luad_s\n",
    "    lengths['all']['hlt'] += hlt_s\n",
    "    lengths['all']['lusc'] += lusc_s\n",
    "    # RNA\n",
    "    probs = [[x,y,z] for x,y,z in zip(df_only['RNA Prob LUAD'], df_only['RNA Prob HLT'], df_only['RNA Prob LUSC'])]\n",
    "    probs = np.asarray(probs)\n",
    "    preds = df_only['RNA Pred'].values\n",
    "    real = df_only['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "    \n",
    "    acc = accuracy_score(real, preds)*100\n",
    "    f1 = f1_score(real, preds, average='weighted')*100\n",
    "    try:\n",
    "        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "        aucs['RNA'].append(auc)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "        auprcs['RNA'].append(np.mean(array))\n",
    "    except:\n",
    "        pass\n",
    "                \n",
    "    accs['RNA'].append(acc)\n",
    "    f1_scores['RNA'].append(f1)\n",
    "    \n",
    "    # WSI\n",
    "    probs = [[x,y,z] for x,y,z in zip(df_only['WSI Prob LUAD'], df_only['WSI Prob HLT'], df_only['WSI Prob LUSC'])]\n",
    "    probs = np.asarray(probs)\n",
    "    preds = df_only['WSI Pred'].values\n",
    "    real = df_only['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "    acc = accuracy_score(real, preds)*100\n",
    "    f1 = f1_score(real, preds, average='weighted')*100\n",
    "    try:\n",
    "        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "        aucs['WSI'].append(auc)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "        auprcs['WSI'].append(np.mean(array))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    accs['WSI'].append(acc)\n",
    "    f1_scores['WSI'].append(f1)\n",
    "    \n",
    "    # miRNA\n",
    "    probs = [[x,y,z] for x,y,z in zip(df_only['miRNA Prob LUAD'], df_only['miRNA Prob HLT'], df_only['miRNA Prob LUSC'])]\n",
    "    probs = np.asarray(probs)\n",
    "    preds = df_only['miRNA Pred'].values\n",
    "    real = df_only['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "    acc = accuracy_score(real, preds)*100\n",
    "    f1 = f1_score(real, preds, average='weighted')*100\n",
    "    try:\n",
    "        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "        aucs['miRNA'].append(auc)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "        auprcs['miRNA'].append(np.mean(array))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    accs['miRNA'].append(acc)\n",
    "    f1_scores['miRNA'].append(f1)\n",
    "    \n",
    "    # CNV\n",
    "    probs = [[x,y,z] for x,y,z in zip(df_only['CNV Prob LUAD'], df_only['CNV Prob HLT'], df_only['CNV Prob LUSC'])]\n",
    "    probs = np.asarray(probs)\n",
    "    preds = df_only['CNV Pred'].values\n",
    "    real = df_only['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "    acc = accuracy_score(real, preds)*100\n",
    "    f1 = f1_score(real, preds, average='weighted')*100\n",
    "    try:\n",
    "        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "        aucs['CNV'].append(auc)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "        auprcs['CNV'].append(np.mean(array))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    accs['CNV'].append(acc)\n",
    "    f1_scores['CNV'].append(f1)\n",
    "    \n",
    "    # DNA\n",
    "    probs = [[x,y,z] for x,y,z in zip(df_only['DNA Prob LUAD'], df_only['DNA Prob HLT'], df_only['DNA Prob LUSC'])]\n",
    "    probs = np.asarray(probs)\n",
    "    preds = df_only['DNA Pred'].values\n",
    "    real = df_only['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "    acc = accuracy_score(real, preds)*100\n",
    "    f1 = f1_score(real, preds, average='weighted')*100\n",
    "    try:\n",
    "        auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "        aucs['DNA'].append(auc)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "        aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "        aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "        auprcs['DNA'].append(np.mean(array))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    accs['DNA'].append(acc)\n",
    "    f1_scores['DNA'].append(f1)\n",
    "    \n",
    "    # integration\n",
    "    probs_int = [[x,y,z] for x,y,z in zip(df_only['Integration Prob LUAD'], df_only['Integration Prob HLT'], df_only['Integration Prob LUSC'])]\n",
    "    preds_int = df_only['Integration Pred'].values\n",
    "    probs_int = np.asarray(probs_int)\n",
    "    real = df_only['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "    acc_int = accuracy_score(real, preds_int)*100\n",
    "    f1_int = f1_score(real, preds_int, average='weighted')*100\n",
    "    try:\n",
    "        auc_int = roc_auc_score(real, probs_int, multi_class='ovr')\n",
    "        aucs['Integration'].append(auc_int)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        aucpr1 = average_precision_score(real_binarized[:, 0], probs_int[:, 0])\n",
    "        aucpr2 = average_precision_score(real_binarized[:, 1], probs_int[:, 1])\n",
    "        aucpr3 = average_precision_score(real_binarized[:, 2], probs_int[:, 2])\n",
    "        array = [x for x in [aucpr1,aucpr2,aucpr3] if not np.isnan(x)]\n",
    "\n",
    "        auprcs['Integration'].append(np.mean(array))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    accs['Integration'].append(acc_int)\n",
    "    f1_scores['Integration'].append(f1_int)\n",
    "\n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5162b7e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for d_type in ['Integration','WSI', 'RNA', 'miRNA', 'CNV', 'DNA']:\n",
    "    print(d_type + ' ACC: {}+-{}'.format(np.mean(accs[d_type]),np.std(accs[d_type])))\n",
    "    print(5*'-')\n",
    "print(10*'-')\n",
    "\n",
    "for d_type in ['Integration','WSI', 'RNA', 'miRNA', 'CNV', 'DNA']:  \n",
    "    print(d_type + ' F1: {}+-{}'.format(np.mean(f1_scores[d_type]),np.std(f1_scores[d_type])))\n",
    "    print(5*'-')\n",
    "print(10*'-')\n",
    "\n",
    "for d_type in ['Integration','WSI', 'RNA', 'miRNA', 'CNV', 'DNA']:\n",
    "    print(d_type + ' AUC: {}+-{}'.format(np.mean(aucs[d_type]),np.std(aucs[d_type])))\n",
    "    print(5*'-')\n",
    "    \n",
    "print(10*'-')\n",
    "\n",
    "for d_type in ['Integration','WSI', 'RNA', 'miRNA', 'CNV', 'DNA']:\n",
    "    print(d_type + ' AUPRC: {}+-{}'.format(np.mean(auprcs[d_type]),np.std(auprcs[d_type])))\n",
    "    print(5*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86721bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lengths['all'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
